<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[安装并配置shadowsocks-libev（yum源方式）]]></title>
    <url>%2F20190617-shadowsocks-libev%2F</url>
    <content type="text"><![CDATA[安装常用工具1yum install git wget vim -y 安装编译工具 CentOS 1yum install epel-release gcc gettext autoconf libtool automake make asciidoc xmlto c-ares-devel libev-devel pcre-devel -y Ubuntu 1sudo apt-get install --no-install-recommends gettext build-essential autoconf libtool libpcre3-dev asciidoc xmlto libev-dev libc-ares-dev automake -y 单独安装 libsodiumlibsodium 是一个先进而且易用的加密库。 主要用于加密、解密、签名和生成密码哈希。 Note：可以点击这里查看 Libsodium 的最新版本。 12345678export LIBSODIUM_VER=1.0.17wget https://download.libsodium.org/libsodium/releases/libsodium-$LIBSODIUM_VER.tar.gztar xvf libsodium-$LIBSODIUM_VER.tar.gzpushd libsodium-$LIBSODIUM_VER./configure --prefix=/usr &amp;&amp; makesudo make installpopdsudo ldconfig 单独安装 mbedtlsSSL和TLS 12345678export MBEDTLS_VER=2.6.0wget https://tls.mbed.org/download/mbedtls-$MBEDTLS_VER-gpl.tgztar xvf mbedtls-$MBEDTLS_VER-gpl.tgzpushd mbedtls-$MBEDTLS_VERmake SHARED=1 CFLAGS=-fPICsudo make DESTDIR=/usr installpopdsudo ldconfig 安装 simple-obfs（已被 v2ray-plugin 取代 ） 混淆插件，如果不需要可以跳过，隐蔽性:v2ray-plugin 优于 simple-obfs；但 V2ray-plugin 延迟较高。 安装编译必须的软件 1yum install zlib-devel openssl-devel -y 安装 simple-obfs 123456git clone https://github.com/shadowsocks/simple-obfs.gitcd simple-obfsgit submodule update --init --recursive./autogen.sh./configure &amp;&amp; makemake install 安装 v2ray-plugin （simple-obfs 和 v2ray-plugin 不能同时开启，安装其中一个即可） 请先安装 SS 后，再来看此连接内容 避免文章内容过长，影响浏览体验，故单独提取到了 gist 上（可能得科学上网才能访问） v2ray-plugin 安装 安装 SS 前面一些环境搭建好了，现在正式安装我们的主角。 (1)下载源码 123git clone https://github.com/shadowsocks/shadowsocks-libev.gitcd shadowsocks-libevgit submodule update --init --recursive (2)编译源码并安装 123cd shadowsocks-libev //确保是在 ss 目录中执行编译./autogen.sh &amp;&amp; ./configure &amp;&amp; makesudo make install // root 权限下安装 (3)创建账号配置文件 12mkdir -p /etc/shadowsocks-libevvim /etc/shadowsocks-libev/config.json 先按 i 键进入编辑模式，然后复制下面内容到文件中，自行替换相应地方。 1234567891011121314&#123; "server":"0.0.0.0", "port_password":&#123; "port1":"pass1", "port2":"pass2" &#125;, "timeout":300, "method":"xchacha20-ietf-poly1305", "mode":"tcp_and_udp", "fast_open": true, "workers": 5, "plugin":"obfs-server", "plugin_opts":"obfs=http;fast-open"&#125; 上面包含了 simple-obfs 混淆插件，如果没有安装该插件，请去掉最后两行。 开放防火墙端口上面设置了一个端口后，现在让防火墙接受该端口号（允许该端口的流量通过防火墙） 启动 Firewalld 防火墙 12systemctl enable firewalldsystemctl start firewalld 添加开放端口(就是上面配置中的端口号) 12firewall-cmd --permanent --zone=public --add-port=port1/tcpfirewall-cmd --permanent --zone=public --add-port=port2/udp 使端口生效 1firewall-cmd --reload 查看已开放的端口（可跳过） 1firewall-cmd --list-all 开启服务到此已经完成所有安装，启动 ss 有两种方式，任选一种方式 是通过使用创建好的配置文件来启动 单个端口号配置：nohup ss-server -c /etc/shadowsocks-libev/config.json &amp; 多个端口号配置：nohup ss-manager -c /etc/shadowsocks-libev/config.json &amp; 是通过将信息写在命令行中启动 1ss-server -s 服务器IP -p 端口 -k 密码 -m chacha20-ietf-poly1305 -u -l 1080 -t 600 --fast-open true --plugin obfs-server --plugin-opts obfs=http;fast-open 加速优化(可选) 加速可选 bbr 和 kcptun Google BBRgoogle bbr 加速优化教程链接：CentOS 安装 BBR Kcptun去 github 下载对应的版本软件：https://github.com/xtaci/kcptun/releases 我是 CentOS7 64 位的系统，所以下载的是 kcptun-linux-amd64-20180305.tar.gz，下载完解压，把 server_linux_amd64 这个文件传到 vps （或者直接通过 vps 下载），然后把该文件移动到 /usr/local/kcptun/ 目录下，如果没有执行权限，记得给该文件赋予执行权限。 创建 kcptun 的配置文件 1vim /usr/local/kcptun/server-config.json 内容如下： 12345678910111213141516&#123; &quot;listen&quot;: &quot;:29900&quot;, &quot;target&quot;: &quot;127.0.0.1:端口&quot;, &quot;key&quot;: &quot;密码&quot;, &quot;crypt&quot;: &quot;aes&quot;, &quot;mode&quot;: &quot;fast2&quot;, &quot;mtu&quot;: 1350, &quot;sndwnd&quot;: 512, &quot;rcvwnd&quot;: 512, &quot;datashard&quot;: 10, &quot;parityshard&quot;: 3, &quot;dscp&quot;: 0, &quot;nocomp&quot;: false, &quot;quiet&quot;: false, &quot;pprof&quot;: false&#125; 后台启动Kcptun server服务 1nohup /usr/local/kcptun/server_linux_amd64 -c /usr/local/kcptun/server-config.json &amp;]]></content>
      <categories>
        <category>vpn</category>
      </categories>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析-DispatcherServlet]]></title>
    <url>%2F20180828-springmvc-analysis%2F</url>
    <content type="text"><![CDATA[SpringMvc是Spring框架的web模块,是目前最常用的web框架之一.下面让我们一起通过阅读源码来揭秘一下他的神奇之处吧. SpringMvc初始化SpringMVC框架是依托于Spring容器。Spring初始化的过程其实就是IoC容器启动的过程，也就是上下文建立的过程。 ServletContext每一个web应用中都有一个Servlet上下文。servlet容器提供一个全局上下文的环境，这个上下文环境将成为其他IoC容器的宿主环境，例如：WebApplicationContext就是作为ServletContext的一个属性存在。 WebApplicationContext在使用SpringMVC的时候，通常需要在web.xml文件中配置： 123&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; ContextLoaderListener 实现了ServletContextListener接口，在SpringMVC中作为监听器的存在，当servlet容器启动时候，会调用contextInitialized进行一些初始化的工作。而ContextLoaderListener中contextInitialized的具体实现在ContextLoader类中的initWebApplicationContext方法中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( "Cannot initialize context because there is already a root application context present - " + "check whether you have multiple ContextLoader* definitions in your web.xml!"); &#125; Log logger = LogFactory.getLog(ContextLoader.class); servletContext.log("Initializing Spring root WebApplicationContext"); if (logger.isInfoEnabled()) &#123; logger.info("Root WebApplicationContext: initialization started"); &#125; long startTime = System.currentTimeMillis(); try &#123; // Store context in local instance variable, to guarantee that // it is available on ServletContext shutdown. if (this.context == null) &#123; this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Published root WebApplicationContext as ServletContext attribute with name [" + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + "]"); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info("Root WebApplicationContext: initialization completed in " + elapsedTime + " ms"); &#125; return this.context; &#125; catch (RuntimeException ex) &#123; logger.error("Context initialization failed", ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125; catch (Error err) &#123; logger.error("Context initialization failed", err); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, err); throw err; &#125;&#125; 上面的部分代码可以看出，初始化时候通过createWebApplicationContext(servletContext);声明一个WebApplicationContext并赋值给ServletContext的org.springframework.web.context.WebApplicationContext.ROOT属性，作为WebApplicationContext的根上下文（root context）。 DispatcherServlet在加载完&lt;context-param&gt;和&lt;listener&gt;之后，容器将加载配置了load-on-startup的servlet。 12345678910&lt;servlet&gt; &lt;servlet-name&gt;example&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;example&lt;/servlet-name&gt; &lt;url-pattern&gt;/example/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; DispatcherServlet在初始化的过程中，会建立一个自己的IoC容器上下文Servlet WebApplicationContext，会以ContextLoaderListener建立的根上下文作为自己的父级上下文。DispatcherServlet持有的上下文默认的实现类是XmlWebApplicationContext。Servlet有自己独有的Bean空间，也可以共享父级上下文的共享Bean，当然也存在配置有含有一个root WebApplicationContext配置。 DispatcherServlet最为SpringMVC核心类，起到了前端控制器（Front controller）的作用，负责请求分发等工作。 从类图中可以看出，DispatcherServlet的继承关系大致如此： 1DispatcherServlet -&gt; FrameworkServlet -&gt; HttpServletBean -&gt; HttpServlet -&gt; GenericServlet 从继承关系上可以得出结论，DispatcherServlet本质上还是一个Servlet。Servlet的生命周期大致分为三个阶段： 初始化阶段 init方法 处理请求阶段 service方法 结束阶段 destroy方法 这里就重点关注DispatcherServlet在这三个阶段具体做了那些工作。 DispatcherServlet初始化DispatcherServlet的init()的实现在其父类HttpServletBean中。 12345678910111213141516171819202122232425262728public final void init() throws ServletException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Initializing servlet '" + getServletName() + "'"); &#125; // Set bean properties from init parameters. try &#123; PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; if (logger.isErrorEnabled()) &#123; logger.error("Failed to set bean properties on servlet '" + getServletName() + "'", ex); &#125; throw ex; &#125; // Let subclasses do whatever initialization they like. initServletBean(); if (logger.isDebugEnabled()) &#123; logger.debug("Servlet '" + getServletName() + "' configured successfully"); &#125;&#125; 以上部分源码描述的过程是通过读取&lt;init-param&gt;的配置元素，读取到DispatcherServlet中，配置相关bean的配置。完成配置后调用initServletBean方法来创建Servlet WebApplicationContext。 initServletBean方法在FrameworkServlet类中重写了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849protected final void initServletBean() throws ServletException &#123; ... try &#123; this.webApplicationContext = initWebApplicationContext(); initFrameworkServlet(); &#125; ...&#125;protected WebApplicationContext initWebApplicationContext() &#123; WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; cwac.setParent(rootContext); &#125; configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; wac = findWebApplicationContext(); &#125; if (wac == null) &#123; wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; onRefresh(wac); &#125; if (this.publishContext) &#123; String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) &#123; this.logger.debug("Published WebApplicationContext of servlet '" + getServletName() + "' as ServletContext attribute with name [" + attrName + "]"); &#125; &#125; return wac;&#125; 上文提到Servlet容器在启动的时候，通过ContextLoaderListener创建一个根上下文，并配置到ServletContext中。可以看出FrameworkServlet这个类做的作用是用来创建WebApplicationContext上下文的。大致过程如下： 首先检查webApplicationContext是否通过构造函数注入，如果有的话，直接使用，并将根上下文设置为父上下文。 如果webApplicationContext没有注入，则检查是否在ServletContext已经注册过，如果已经注册过，直接返回使用。 如果没有注册过，将重新新建一个webApplicationContext。将根上下文设置为父级上下文。 不管是何种策略获取的webApplicationContext，都将会调用onRefresh方法，onRefresh方法会调用initStrategies方法，通过上下文初始化HandlerMappings、HandlerAdapters、ViewResolvers等等。 最后，同样会将所得webApplicationContext注册到ServletContext中。 而initFrameworkServlet()默认的实现是空的。这也可算是SpingMVC留的一个扩展点。 DispatcherServlet处理请求纵观SpringMVC的源码，大量运用模板方法的设计模式。Servlet的service方法也不例外。FrameworkServlet类重写service方法： 123456789101112@Overrideprotected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpMethod httpMethod = HttpMethod.resolve(request.getMethod()); if (HttpMethod.PATCH == httpMethod || httpMethod == null) &#123; processRequest(request, response); &#125; else &#123; super.service(request, response); &#125;&#125; 如果请求的方法是PATCH或者空，直接调用processRequest方法（后面会详细解释）；否则，将调用父类的service的方法，即HttpServlet的service方法, 而这里会根据请求方法，去调用相应的doGet、doPost、doPut…… 而doXXX系列方法的实现并不是HttpServlet类中，而是在FrameworkServlet类中。在FrameworkServlet中doXXX系列实现中，都调用了上面提到的processRequest方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); LocaleContext localeContext = buildLocaleContext(request); RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor()); initContextHolders(request, localeContext, requestAttributes); try &#123; doService(request, response); &#125; catch (ServletException | IOException ex) &#123; failureCause = ex; throw ex; &#125; catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException("Request processing failed", ex); &#125; finally &#123; resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; if (logger.isDebugEnabled()) &#123; if (failureCause != null) &#123; this.logger.debug("Could not complete request", failureCause); &#125; else &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; logger.debug("Leaving response open for concurrent processing"); &#125; else &#123; this.logger.debug("Successfully completed request"); &#125; &#125; &#125; publishRequestHandledEvent(request, response, startTime, failureCause); &#125;&#125; 为了避免子类重写它，该方法用final修饰。 首先调用initContextHolders方法,将获取到的localeContext、requestAttributes、request绑定到线程上。 然后调用doService方法，doService具体是由DispatcherServlet类实现的。 doService执行完成后，调用resetContextHolders，解除localeContext等信息与线程的绑定。 最终调用publishRequestHandledEvent发布一个处理完成的事件。 DispatcherServlet类中的doService方法实现会调用doDispatch方法，这里请求分发处理的主要执行逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = "GET".equals(method); if (isGet || "HEAD".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug("Last-Modified value for [" + getRequestUri(request) + "] is: " + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException("Handler dispatch failed", err); &#125; processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException("Handler processing failed", err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; &#125; doDispatch主要流程是： 先判断是否Multipart类型的请求。如果是则通过multipartResolver解析request 通过getHandler方法找到从HandlerMapping找到该请求对应的handler,如果没有找到对应的handler则抛出异常。 通过getHandlerAdapter方法找到handler对应的HandlerAdapter 如果有拦截器，执行拦截器preHandler方法 HandlerAdapter执行handle方法处理请求，返回ModelAndView。 如果有拦截器，执行拦截器postHandle方法 然后调用processDispatchResult方法处理请求结果，封装到response中。 SpingMVC 请求处理流程SpringMVC框架是围绕DispatcherServlet设计的。DispatcherServlet负责将请求分发给对应的处理程序。从网上找了两个图，可以大致了解SpringMVC的框架对请求的处理流程。 用户发送请求，Front Controller（DispatcherServlet）根据请求信息将请求委托给对应的Controller进行处理。 DispatcherServlet接收到请求后，HandlerMapping将会把请求封装为HandlerExecutionChain，而HandlerExecutionChain包含请求的所有信息，包括拦截器、Handler处理器等。 DispatcherServlet会找到对应的HandlerAdapter，并调用对应的处理方法，并返回一个ModelAndView对象。 DispatcherServlet会将ModelAndView对象传入View层进行渲染。 最终DispatcherServlet将渲染好的response返回给用户。 总结本文主要分析SpringMVC中DispatcherServlet的初始化、请求流传过程等。 发现了SpringMVC中在DispatcherServlet的实现过程中运用了模板方法设计模式，看到SpringMVC中留给用户可扩展的点也有很多，体会到Open for extension, closed for modification的设计原则。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用技巧]]></title>
    <url>%2F20180704-git%2F</url>
    <content type="text"><![CDATA[命令别名平时不太用GUI，都是在IDEA中直接敲命令进行相关操作，觉得命令行相对于GUI来说效率还是要高一些，一开始老老实实的敲完整的命令，比如 123git checkout devgit statusgit commit -m 'update' 后来才发现，Git有别名这一功能，通过下面几个命令可以把比较长的命令使用简短两个字母代替。 1234git config --global alias.co checkoutgit config --global alias.ci commitgit config --global alias.br branchgit config --global alias.st status 然后就可以愉快的敲 git st 来查看当前状态了，除了这种简单的替换，还可以进行复杂的替换，比如要显示一个牛逼的提交记录，可以把lg替换成很长的一段命令组合，如下： 1git config --global alias.lg log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit 配置完了,直接输入 git lg 如何放弃本地修改这个功能平时用的还是比较多的，经常功能加了之后发现并没什么卵用，需要放弃这些修改，不过这也分为好几种情况。 1、未进行add操作这种情况，应该是最常见的，一般修改不会轻易进行git add &lt;file&gt;操作，这个时候只需要进行简单的 git checkout -- . 撤销全部, 或者 git checkout -- &lt;file&gt; 恢复具体的文件。 命令中的–很重要，没有–，就可能变成 “分支切换” 的命令，但是如果没写–，且没有对应的分支，也可以恢复，为了保险，还是老老实实加上。 该命令就是用来放弃那些还未加入缓存区的修改操作，包括文件内容的修改和文件的删除，但是对于那些新建的文件来说，并不把它删掉，因为刚新建的文件对于Git来说是未知的，需要手动的删除。 2、已经被add进缓存好吧，这种情况，我也是经常遇到，因为手速太快，执行完git add . 之后，发现一些有问题，怎么办？ 因为这些修改已经被放入缓存区了，git checkout 操作已经无力回天，这个时候，可以使用 git reset HEAD &lt;file&gt; 命令来放弃指定的文件的缓存，如果要放弃所有修改的缓存，可以使用 git reset HEAD . 该命令相当于撤销 git add 的影响，在执行该命令后，本地的修改并不会消失，而是回到add之前的状态。 3、add完之后，还commit了 这种情况最糟心，还好没有push到远程仓库，还可以抢救。其实方法也很简单，可以使用 git reset --hard HEAD^ 来回退到上一次commit的状态。 另外使用此命令可以回退到任意版本：git reset --hard commitid 这里的commitId就是使用 git log 显示的提交历史中的SHA码，只需要前面几位即可。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[揭秘ThreadLocal]]></title>
    <url>%2F20180627-ThreadLocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal是开发中最常用的技术之一，也是面试重要的考点。本文将由浅入深，介绍ThreadLocal的使用方式、实现原理、内存泄漏问题以及使用场景。 ThreadLocal作用在并发编程中时常有这样一种需求：每条线程都需要存取一个同名变量，但每条线程中该变量的值均不相同。 如果是你，该如何实现上述功能？常规的思路如下： 使用一个线程共享的Map&lt;Thread,Object&gt;，Map中的key为线程对象，value即为需要存储的值。那么，我们只需要通过map.get(Thread.currentThread())即可获取本线程中该变量的值。 这种方式确实可以实现我们的需求，但它有何缺点呢？——答案就是：需要同步，效率低！ 由于这个map对象需要被所有线程共享，因此需要加锁来保证线程安全性。当然我们可以使用java.util.concurrent.*包下的ConcurrentHashMap提高并发效率，但这种方法只能降低锁的粒度，不能从根本上避免同步锁。而JDK提供的ThreadLocal就能很好地解决这一问题。下面来看看ThreadLocal是如何高效地实现这一需求的。 如何使用ThreadLocal在介绍ThreadLocal原理之前，首先简单介绍一下它的使用方法。 12345678910111213141516public class Main&#123; private ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;(); public void start() &#123; for (int i=0; i&lt;10; i++) &#123; new Thread(new Runnable()&#123; @override public void run()&#123; threadLocal.set(i); threadLocal.get(); threadLocal.remove(); &#125; &#125;).start(); &#125; &#125;&#125; 首先我们需要创建一个线程共享的ThreadLocal对象，该对象用于存储Integer类型的值； 然后在每条线程中可以通过如下方法操作ThreadLocal： set(obj)：向当前线程中存储数据 get()：获取当前线程中的数据 remove()：删除当前线程中的数据 ThreadLocal的使用方法非常简单，关键在于它背后的实现原理。回到上面的问题：ThreadLocal究竟是如何避免同步锁，从而保证读写的高效？ ThreadLocal并不维护ThreadLocalMap，并不是一个存储数据的容器，它只是相当于一个工具包，提供了操作该容器的方法，如get、set、remove等。而ThreadLocal内部类ThreadLocalMap才是存储数据的容器，并且该容器由Thread维护。 每一个Thread对象均含有一个ThreadLocalMap类型的成员变量threadLocals，它存储本线程中所有ThreadLocal对象及其对应的值。 ThreadLocalMap由一个个Entry对象构成，Entry的代码如下： 12345678static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; Entry继承自WeakReference&lt;ThreadLocal&lt;?&gt;&gt;，一个Entry由ThreadLocal对象和Object构成。由此可见，Entry的key是ThreadLocal对象，并且是一个弱引用。当没指向key的强引用后，该key就会被垃圾收集器回收。 那么，ThreadLocal是如何工作的呢？下面来看set和get方法。 1234567891011121314151617181920212223242526public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 当执行set方法时，ThreadLocal首先会获取当前线程对象，然后获取当前线程的ThreadLocalMap对象。再以当前ThreadLocal对象为key，将值存储进ThreadLocalMap对象中。 get方法执行过程类似。ThreadLocal首先会获取当前线程对象，然后获取当前线程的ThreadLocalMap对象。再以当前ThreadLocal对象为key，获取对应的value。 由于每一条线程均含有各自私有的ThreadLocalMap容器，这些容器相互独立互不影响，因此不会存在线程安全性问题，从而也无需使用同步机制来保证多条线程访问容器的互斥性 为何要使用弱引用？对弱引用不了解的同学可以参考笔者的另一篇文章：http://blog.csdn.net/u010425776/article/details/50760053。 Java设计之初的一大宗旨就是——弱化指针。 Java设计者希望通过合理的设计简化编程，让程序员无需处理复杂的指针操作。然而指针是客观存在的，在目前的Java开发中也不可避免涉及到“指针操作”。如： 1Object a = new Object(); 上述代码创建了一个强引用a，只要强引用存在，垃圾收集器是不会回收该对象的。如果该对象非常庞大，那么为了节约内存空间，在该对象使用完成后，我们需要手动拆除该强引用，如下面代码所示： 1a = null; 此时，指向该对象的强引用消除了，垃圾收集器便可以回收该对象。但在这个过程中，仍然需要程序员处理指针。为了弱化指针这一概念，弱引用便出现了，如下代码创建了一个Person类型的弱引用： 1WeakReference&lt;Person&gt; wr = new WeakReference&lt;Person&gt;(new Person()); 此时程序员不用再关注指针，只要没有强引用指向Person对象，垃圾收集器每次运行都会自动将该对象释放。 那么，ThreadLocalMap中的key使用弱引用的原因也是如此。当一条线程中的ThreadLocal对象使用完毕，没有强引用指向它的时候，垃圾收集器就会自动回收这个Key，从而达到节约内存的目的。 那么，问题又来了——这会导致内存泄漏问题！ ThreadLocal的内存泄漏问题 在ThreadLocalMap中，只有key是弱引用，value仍然是一个强引用。当某一条线程中的ThreadLocal使用完毕，没有强引用指向它的时候，这个key指向的对象就会被垃圾收集器回收，从而这个key就变成了null；然而，此时value和value指向的对象之间仍然是强引用关系，只要这种关系不解除，value指向的对象永远不会被垃圾收集器回收，从而导致内存泄漏！ 不过不用担心，ThreadLocal提供了这个问题的解决方案。 每次操作set、get、remove操作时，ThreadLocal都会将key为null的Entry删除，从而避免内存泄漏。 那么问题又来了，如果一个线程运行周期较长，而且将一个大对象放入LocalThreadMap后便不再调用set、get、remove方法，此时该仍然可能会导致内存泄漏。 这个问题确实存在，没办法通过ThreadLocal解决，而是需要程序员在完成ThreadLocal的使用后要养成手动调用remove的习惯，从而避免内存泄漏。 ThreadLocal的使用场景Web系统Session的存储就是ThreadLocal一个典型的应用场景。 Web容器采用线程隔离的多线程模型，也就是每一个请求都会对应一条线程，线程之间相互隔离，没有共享数据。这样能够简化编程模型，程序员可以用单线程的思维开发这种多线程应用。 当请求到来时，可以将当前Session信息存储在ThreadLocal中，在请求处理过程中可以随时使用Session信息，每个请求之间的Session信息互不影响。当请求处理完成后通过remove方法将当前Session信息清除即可。 转载地址: https://www.jianshu.com/p/3f3620f9011d]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性——StreamAPI(二)]]></title>
    <url>%2F20180626-lambda-stream-2%2F</url>
    <content type="text"><![CDATA[1. 收集器简介收集器用来将经过筛选、映射的流进行最后的整理，可以使得最后的结果以不同的形式展现。 collect方法即为收集器，它接收Collector接口的实现作为具体收集器的收集方法。 Collector接口提供了很多默认实现的方法，我们可以直接使用它们格式化流的结果；也可以自定义Collector接口的实现，从而定制自己的收集器。 这里先介绍Collector常用默认静态方法的使用，自定义收集器会在下一篇博文中介绍。 2. 收集器的使用2.1 归约流由一个个元素组成，归约就是将一个个元素“折叠”成一个值，如求和、求最值、求平均值都是归约操作。 2.1.1 计数12long count = list.stream() .collect(Collectors.counting()); 也可以不使用收集器的计数函数： 1long count = list.stream().count(); 注意：计数的结果一定是long类型。 2.1.2 最值 例：找出所有人中年龄最大的人 12Optional&lt;Person&gt; oldPerson = list.stream() .collect(Collectors.maxBy(Comparator.comparingInt(Person::getAge))); 计算最值需要使用Collector.maxBy和Collector.minBy，这两个函数需要传入一个比较器Comparator.comparingInt，这个比较器又要接收需要比较的字段。 这个收集器将会返回一个Optional类型的值。 2.1.3 求和 例：计算所有人的年龄总和 12int summing = list.stream() .collect(Collectors.summingInt(Person::getAge)); 当然，既然Java8提供了summingInt，那么还提供了summingLong、summingDouble。 2.1.4 求平均值 例：计算所有人的年龄平均值 12double avg = list.stream() .collect(Collectors.averagingInt(Person::getAge)); 注意：计算平均值时，不论计算对象是int、long、double，计算结果一定都是double。 2.1.5 一次性计算所有归约操作Collectors.summarizingInt函数能一次性将最值、均值、总和、元素个数全部计算出来，并存储在对象IntSummaryStatisics中。 可以通过该对象的getXXX()函数获取这些值。 2.1.6 连接字符串 例：将所有人的名字连接成一个字符串 12String names = list.stream() .collect(Collectors.joining()); 每个字符串默认分隔符为空格，若需要指定分隔符，则在joining中加入参数即可： 12String names = list.stream() .collect(Collectors.joining(&quot;, &quot;)); 此时字符串之间的分隔符为逗号。 2.1.7 一般性的归约操作若你需要自定义一个归约操作，那么需要使用Collectors.reducing函数，该函数接收三个参数： 第一个参数为归约的初始值 第二个参数为归约操作进行的字段 第三个参数为归约操作的过程 例：计算所有人的年龄总和 12Optional&lt;Integer&gt; sumAge = list.stream() .collect(Collectors.reducing(0,Person::getAge,(i,j)-&gt;i+j)); 上面例子中，reducing函数一共接收了三个参数： 第一个参数表示归约的初始值。我们需要累加，因此初始值为0 第二个参数表示需要进行归约操作的字段。这里我们对Person对象的age字段进行累加。 第三个参数表示归约的过程。这个参数接收一个Lambda表达式，而且这个Lambda表达式一定拥有两个参数，分别表示当前相邻的两个元素。由于我们需要累加，因此我们只需将相邻的两个元素加起来即可。 Collectors.reducing方法还提供了一个单参数的重载形式。 你只需传一个归约的操作过程给该方法即可(即第三个参数)，其他两个参数均使用默认值。 第一个参数默认为流的第一个元素 第二个参数默认为流的元素 这就意味着，当前流的元素类型为数值类型，并且是你要进行归约的对象。 例：采用单参数的reducing计算所有人的年龄总和 123Optional&lt;Integer&gt; sumAge = list.stream() .filter(Person::getAge) .collect(Collectors.reducing((i,j)-&gt;i+j)); 2.2 分组分组就是将流中的元素按照指定类别进行划分，类似于SQL语句中的GROUPBY。 2.2.1 一级分组 例：将所有人分为老年人、中年人、青年人 123456789Map&lt;String,List&lt;Person&gt;&gt; result = list.stream() .collect(Collectors.groupingby((person)-&gt;&#123; if(person.getAge()&gt;60) return &quot;老年人&quot;; else if(person.getAge()&gt;40) return &quot;中年人&quot;; else return &quot;青年人&quot;; &#125;)); groupingby函数接收一个Lambda表达式，该表达式返回String类型的字符串，groupingby会将当前流中的元素按照Lambda返回的字符串进行分组。 分组结果是一个Map&lt; String,List&lt; Person&gt;&gt;，Map的键就是组名，Map的值就是该组的Perosn集合。 2.2.2 多级分组多级分组可以支持在完成一次分组后，分别对每个小组再进行分组。 使用具有两个参数的groupingby重载方法即可实现多级分组。 第一个参数：一级分组的条件 第二个参数：一个新的groupingby函数，该函数包含二级分组的条件 例：将所有人分为老年人、中年人、青年人，并且将每个小组再分成：男女两组。 12345678910Map&lt;String,Map&lt;String,List&lt;Person&gt;&gt;&gt; result = list.stream() .collect(Collectors.groupingby((person)-&gt;&#123; if(person.getAge()&gt;60) return &quot;老年人&quot;; else if(person.getAge()&gt;40) return &quot;中年人&quot;; else return &quot;青年人&quot;; &#125;, groupingby(Person::getSex))); 此时会返回一个非常复杂的结果：Map&lt; String,Map&lt; String,List&lt; Person&gt;&gt;&gt;。 2.2.3 对分组进行统计拥有两个参数的groupingby函数不仅仅能够实现多几分组，还能对分组的结果进行统计。 例：统计每一组的人数 12345678910Map&lt;String,Long&gt; result = list.stream() .collect(Collectors.groupingby((person)-&gt;&#123; if(person.getAge()&gt;60) return &quot;老年人&quot;; else if(person.getAge()&gt;40) return &quot;中年人&quot;; else return &quot;青年人&quot;; &#125;, counting())); 此时会返回一个Map&lt; String,Long&gt;类型的map，该map的键为组名，map的值为该组的元素个数。 将收集器的结果转换成另一种类型当使用maxBy、minBy统计最值时，结果会封装在Optional中，该类型是为了避免流为空时计算的结果也为空的情况。在单独使用maxBy、minBy函数时确实需要返回Optional类型，这样能确保没有空指针异常。然而当我们使用groupingBy进行分组时，若一个组为空，则该组将不会被添加到Map中，从而Map中的所有值都不会是一个空集合。既然这样，使用maxBy、minBy方法计算每一组的最值时，将结果封装在optional对象中就显得有些多余。 我们可以使用collectingAndThen函数包裹maxBy、minBy，从而将maxBy、minBy返回的Optional对象进行转换。 例：将所有人按性别划分，并计算每组最大的年龄。 123456Map&lt;String,Integer&gt; map = list.stream() .collect(groupingBy(Person::getSex, collectingAndThen( maxBy(comparingInt(Person::getAge)), Optional::get ))); 此时返回的是一个Map&lt; String,Integer&gt;，String表示每组的组名(男、女)，Integer为每组最大的年龄。 如果不用collectingAndThen包裹maxBy，那么最后返回的结果为Map&lt; String,Optional&lt; Person&gt;&gt;。 使用collectingAndThen包裹maxBy后，首先会执行maxBy函数，该函数执行完后便会执行Optional::get，从而将Optional中的元素取出来。 2.3 分区分区是分组的一种特殊情况，它只能分成true、false两组。 分组使用partitioningBy方法，该方法接收一个Lambda表达式，该表达是必须返回boolean类型，partitioningBy方法会将Lambda返回结果为true和false的元素各分成一组。 partitioningBy方法返回的结果为Map&lt; Boolean,List&lt; T&gt;&gt;。 此外，partitioningBy方法和groupingBy方法一样，也可以接收第二个参数，实现二级分区或对分区结果进行统计。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java8</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性——StreamAPI(一)]]></title>
    <url>%2F20180625-lambda-stream-1%2F</url>
    <content type="text"><![CDATA[1. 流的基本概念1.1 什么是流？流是Java8引入的全新概念，它用来处理集合中的数据，暂且可以把它理解为一种高级集合。 众所周知，集合操作非常麻烦，若要对集合进行筛选、投影，需要写大量的代码，而流是以声明的形式操作集合，它就像SQL语句，我们只需告诉流需要对集合进行什么操作，它就会自动进行操作，并将执行结果交给你，无需我们自己手写代码。 因此，流的集合操作对我们来说是透明的，我们只需向流下达命令，它就会自动把我们想要的结果给我们。由于操作过程完全由Java处理，因此它可以根据当前硬件环境选择最优的方法处理，我们也无需编写复杂又容易出错的多线程代码了。 1.2 流的特点 只能遍历一次 我们可以把流想象成一条流水线，流水线的源头是我们的数据源(一个集合)，数据源中的元素依次被输送到流水线上，我们可以在流水线上对元素进行各种操作。一旦元素走到了流水线的另一头，那么这些元素就被“消费掉了”，我们无法再对这个流进行操作。当然，我们可以从数据源那里再获得一个新的流重新遍历一遍。 采用内部迭代方式 若要对集合进行处理，则需我们手写处理代码，这就叫做外部迭代。而要对流进行处理，我们只需告诉流我们需要什么结果，处理过程由流自行完成，这就称为内部迭代。 1.3 流的操作种类流的操作分为两种，分别为中间操作 和 终端操作。 中间操作 当数据源中的数据上了流水线后，这个过程对数据进行的所有操作都称为“中间操作”。 中间操作仍然会返回一个流对象，因此多个中间操作可以串连起来形成一个流水线。 终端操作 当所有的中间操作完成后，若要将数据从流水线上拿下来，则需要执行终端操作。 终端操作将返回一个执行结果，这就是你想要的数据。 1.4 流的操作过程使用流一共需要三步： 准备一个数据源 执行中间操作 中间操作可以有多个，它们可以串连起来形成流水线。 执行终端操作 执行终端操作后本次流结束，你将获得一个执行结果。 2. 流的使用2.1 获取流在使用流之前，首先需要拥有一个数据源，并通过StreamAPI提供的一些方法获取该数据源的流对象。数据源可以有多种形式： 集合 这种数据源较为常用，通过stream()方法即可获取流对象： 12List&lt;Person&gt; list = new ArrayList&lt;Person&gt;(); Stream&lt;Person&gt; stream = list.stream(); 数组 通过Arrays类提供的静态函数stream()获取数组的流对象： 12String[] names = &#123;&quot;chaimm&quot;,&quot;peter&quot;,&quot;john&quot;&#125;;Stream&lt;String&gt; stream = Arrays.stream(names); 值 直接将几个值变成流对象： 1Stream&lt;String&gt; stream = Stream.of(&quot;chaimm&quot;,&quot;peter&quot;,&quot;john&quot;); 文件 try(Stream lines = Files.lines(Paths.get(“文件路径名”),Charset.defaultCharset())){ //可对lines做一些操作 }catch(IOException e){ } PS：Java7简化了IO操作，把打开IO操作放在try后的括号中即可省略关闭IO的代码。 2.2 筛选filterfilter函数接收一个Lambda表达式作为参数，该表达式返回boolean，在执行过程中，流将元素逐一输送给filter，并筛选出执行结果为true的元素。 如，筛选出所有学生： 123List&lt;Person&gt; result = list.stream() .filter(Person::isStudent) .collect(toList()); 2.3 去重distinct去掉重复的结果： 123List&lt;Person&gt; result = list.stream() .distinct() .collect(toList()); 2.4 截取截取流的前N个元素： 123List&lt;Person&gt; result = list.stream() .limit(3) .collect(toList()); 2.5 跳过跳过流的前n个元素： 123List&lt;Person&gt; result = list.stream() .skip(3) .collect(toList()); 2.6 映射对流中的每个元素执行一个函数，使得元素转换成另一种类型输出。流会将每一个元素输送给map函数，并执行map中的Lambda表达式，最后将执行结果存入一个新的流中。 如，获取每个人的姓名(实则是将Perosn类型转换成String类型)： 123List&lt;Person&gt; result = list.stream() .map(Person::getName) .collect(toList()); 2.7 合并多个流 例：列出List中各不相同的单词，List集合如下： 1234List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;I am a boy&quot;);list.add(&quot;I love the girl&quot;);list.add(&quot;But the girl loves another girl&quot;); 思路如下： 首先将list变成流： 1list.stream(); 按空格分词： 12list.stream() .map(line-&gt;line.split(&quot; &quot;)); 分完词之后，每个元素变成了一个String[]数组。 将每个String[]变成流： 123list.stream() .map(line-&gt;line.split(&quot; &quot;)) .map(Arrays::stream) 此时一个大流里面包含了一个个小流，我们需要将这些小流合并成一个流。 将小流合并成一个大流： 用flagmap替换刚才的map 123list.stream() .map(line-&gt;line.split(&quot; &quot;)) .flagmap(Arrays::stream) 去重 12345list.stream() .map(line-&gt;line.split(&quot; &quot;)) .flagmap(Arrays::stream) .distinct() .collect(toList()); 2.8 是否匹配任一元素：anyMatchanyMatch用于判断流中是否存在至少一个元素满足指定的条件，这个判断条件通过Lambda表达式传递给anyMatch，执行结果为boolean类型。 如，判断list中是否有学生： 12boolean result = list.stream() .anyMatch(Person::isStudent); 2.9 是否匹配所有元素：allMatchallMatch用于判断流中的所有元素是否都满足指定条件，这个判断条件通过Lambda表达式传递给anyMatch，执行结果为boolean类型。 如，判断是否所有人都是学生： 12boolean result = list.stream() .allMatch(Person::isStudent); 2.10 是否未匹配所有元素：noneMatchnoneMatch与allMatch恰恰相反，它用于判断流中的所有元素是否都不满足指定条件： 12boolean result = list.stream() .noneMatch(Person::isStudent); 2.11 获取任一元素findAnyfindAny能够从流中随便选一个元素出来，它返回一个Optional类型的元素。 12Optional&lt;Person&gt; person = list.stream() .findAny(); Optional介绍Optional是Java8新加入的一个容器，这个容器只存1个或0个元素，它用于防止出现NullpointException，它提供如下方法： isPresent() 判断容器中是否有值。 ifPresent(Consume lambda) 容器若不为空则执行括号中的Lambda表达式。 T get() 获取容器中的元素，若容器为空则抛出NoSuchElement异常。 T orElse(T other) 获取容器中的元素，若容器为空则返回括号中的默认值。 2.12 获取第一个元素findFirst12Optional&lt;Person&gt; person = list.stream() .findFirst(); 2.13 归约归约是将集合中的所有元素经过指定运算，折叠成一个元素输出，如：求最值、平均数等，这些操作都是将一个集合的元素折叠成一个元素输出。 在流中，reduce函数能实现归约。 reduce函数接收两个参数： 初始值 进行归约操作的Lambda表达式 2.13.1 元素求和：自定义Lambda表达式实现求和例：计算所有人的年龄总和 1int age = list.stream().reduce(0, (person1,person2)-&gt;person1.getAge()+person2.getAge()); reduce的第一个参数表示初试值为0； reduce的第二个参数为需要进行的归约操作，它接收一个拥有两个参数的Lambda表达式，reduce会把流中的元素两两输给Lambda表达式，最后将计算出累加之和。 2.13.2 元素求和：使用Integer.sum函数求和上面的方法中我们自己定义了Lambda表达式实现求和运算，如果当前流的元素为数值类型，那么可以使用Integer提供了sum函数代替自定义的Lambda表达式，如： 1int age = list.stream().reduce(0, Integer::sum); Integer类还提供了min、max等一系列数值操作，当流中元素为数值类型时可以直接使用。 2.14 数值流的使用采用reduce进行数值操作会涉及到基本数值类型和引用数值类型之间的装箱、拆箱操作，因此效率较低。 当流操作为纯数值操作时，使用数值流能获得较高的效率。 2.14.1 将普通流转换成数值流StreamAPI提供了三种数值流：IntStream、DoubleStream、LongStream，也提供了将普通流转换成数值流的三种方法：mapToInt、mapToDouble、mapToLong。 如，将Person中的age转换成数值流： 12IntStream stream = list.stream() .mapToInt(Person::getAge); 2.14.2 数值计算每种数值流都提供了数值计算函数，如max、min、sum等。 如，找出最大的年龄： 123OptionalInt maxAge = list.stream() .mapToInt(Person::getAge) .max(); 由于数值流可能为空，并且给空的数值流计算最大值是没有意义的，因此max函数返回OptionalInt，它是Optional的一个子类，能够判断流是否为空，并对流为空的情况作相应的处理。 此外，mapToInt、mapToDouble、mapToLong进行数值操作后的返回结果分别为：OptionalInt、OptionalDouble、OptionalLong]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java8</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性——Lambda表达式]]></title>
    <url>%2F20180624-lambda%2F</url>
    <content type="text"><![CDATA[1. 预备知识在了解Lambda表达式之前首先需要了解以下预备知识。 1.1 如何成为一名高级码农？如果老板让你写一个用于毁灭地球的函数，而你写了一个毁灭行星的函数，若要毁灭地球只需将毁灭地球的过程传递给“毁灭行星”。能做到这一点，你就是一名高级码农。 以上这个问题体现了程序可扩展性的思想。总结一下，要成为一名高级码农，在编码的时候要以发展的眼光看待问题。一个具体问题到来的时候，你需要基于当前问题抽象出解决这一类问题的办法，那么当相似的问题到来时，你只需花少量的时间就能完成任务，而且避免了代码复制，降低了bug的风险。 1.2 实现程序可扩展性的例子在Java8以前，要实现程序的可扩展性，我们常用匿名内部类传递用于扩展的代码，举个例子： 实现一个能从List集合中筛选指定对象的函数filter。 若List中存放Person对象，要求筛选出30岁以上的Person 若List中存放Apple，要求筛选出红色的Apple 使用匿名内部类的解决方案： 实现filter函数 12345678List&lt;T&gt; filter(List&lt;T&gt; list, FilterProcessor filterProcessor)&#123; List&lt;T&gt; result = new ArrayList&lt;T&gt;(); for(T t : list)&#123; if(filterProcessor.process(t)) result.add(t); &#125; return list;&#125; 定义FilterProcessor接口 123interface FilterProcessor&lt;T&gt;&#123; boolean process(T);&#125; 使用匿名内部类实现筛选出30岁以上的Person 1234567List&lt;Person&gt; result = filter(list, new FilterProcessor&lt;Person&gt;()&#123; boolean process(Person person)&#123; if(person.getAge()&gt;=30) return true; return false; &#125;&#125;); 使用匿名内部类筛选出红色的Apple 1234567List&lt;Apple&gt; result = filter(list, new FilterProcessor&lt;Apple&gt;()&#123; boolean process(Apple apple)&#123; if(apple.getColor().equals("red")) return true; return false; &#125;&#125;); 若要增加新的筛选规则，只需给filter函数传递新的匿名内部类即可。程序具有了可扩展性，恭喜你已经成为一名高级码农！ 1.3 什么是“策略模式”？上述过程总结来说，当我们需要解决同一类问题的时候，若发现这类问题大部分处理过程是一致的，只是核心步骤存在差异，这时候就可以使用上述方式：先把函数相同的部分写好，再抽象出一个接口，不同部分的代码放在接口的实现类中。要使用时，只需将实现类的对象传递给该函数即可。 这种方式在设计模式中称为“策略模式”。 1.4 使用匿名内部类实现策略模式的弊端？使用匿名内部类实现策略模式代码比较冗余，不易阅读，就像这样： 1234567List&lt;Apple&gt; result = filter(list, new FilterProcessor&lt;Apple&gt;()&#123; boolean process(Apple apple)&#123; if(apple.getColor().equals("red")) return true; return false; &#125;&#125;); Java8提出了Lambda表达式，它是实现策略模式的另外一种方式，目的就是为了使代码简单清晰。 使用Lambda表达式实现相同功能： 1List&lt;Apple&gt; result = filter(list, (Apple apple)—&gt;apple.getColor().equals("red")); 这样是不是简单多了！Lambda表达式本质上是将一个函数的代码作为一个参数或变量进行传递，这种处理方式有个专门的名字——函数式编程。 1.5 什么是函数式编程？所谓“函数式”编程，就是将函数的代码当作一个变量，传递给另一个变量或传递给一个函数，这种编程方式就称为“函数式编程”。 Java8中采用Lambda表达式实现函数式编程，它是策略模式的第二种实现方式，目的就是简化策略模式的代码实现。 2. Lambda表达式的语法Lambda表达式用于表示一个函数，所以它和函数一样，也拥有参数、返回值、函数体，但它没有函数名，所以Lambda表达式相当于一个匿名函数。语法如下： 1(Person person)—&gt;person.getAge()&gt;30 Lambda表达式用—&gt;连接，-&gt;左侧为函数的参数，-&gt;右侧为函数体。 若右侧由多条语句构成则需要用{}包裹，如： 1(Person person)—&gt;&#123;person.getAge()&gt;30;System.out.println(person.getName());&#125; Lambda表达式无需显示指定返回值类型，JVM会根据-&gt;右侧语句的返回结果自动判断返回值类型，如： 1(Person person)—&gt;person.getAge()&gt;30 #自动判断返回值为boolean型 3. 如何使用Lambda表达式？ 仍以筛选年龄大于30的Person对象为例： 3.1 为Lambda表达式定义函数式接口1234@FunctionalInterfaceinterface FilterProcessor&lt;T&gt;&#123; boolean process(T t);&#125; PS：该接口只能有一个抽象函数！接下来Lambda表达式就是该抽象函数的实现。 PS：在为Lambda表达式定义函数式接口时，需要加上注解@FunctionalInterface，这样当该接口中抽象函数个数不是1时就会报错提示。 3.2 实现筛选函数123456List&lt;T&gt; filter(List&lt;T&gt; list, FilterProcessor&lt;T&gt; filterProcessor)&#123; List&lt;T&gt; result = new ArrayList&lt;T&gt;(); if(filterProcessor.process(t)) result.add(t); return result;&#125; filter函数接收一个函数式接口，该参数用于接收一个Lambda表达式。 3.3 传递Lambda表达式1List&lt;Person&gt; result = filter(list, (Person p)-&gt;p.getAge()&gt;30); 直接将Lambda表达式作为参数传递给filter的函数式接口即可，从而在result中就能获取年龄超过30岁的Person对象。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java8</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库SQL语句优化]]></title>
    <url>%2F20180323-sql%2F</url>
    <content type="text"><![CDATA[一、问题的提出 在应用系统开发初期，由于开发数据库数据比较少，对于查询SQL语句，复杂视图的的编写等体会不出SQL语句各种写法的性能优劣，但是如果将应用系统提交实际应用后，随着数据库中数据的增加，系统的响应速度就成为目前系统需要解决的最主要的问题之一。系统优化中一个很重要的方面就是SQL语句的优化。对于海量数据，劣质SQL语句和优质SQL语句之间的速度差别可以达到上百倍，可见对于一个系统不是简单地能实现其功能就可，而是要写出高质量的SQL语句，提高系统的可用性。 在多数情况下，Oracle使用索引来更快地遍历表，优化器主要根据定义的索引来提高性能。但是，如果在SQL语句的where子句中写的SQL代码不合理，就会造成优化器删去索引而使用全表扫描，一般就这种SQL语句就是所谓的劣质SQL语句。在编写SQL语句时我们应清楚优化器根据何种原则来删除索引，这有助于写出高性能的SQL语句。 二、SQL语句编写注意问题 下面就某些SQL语句的where子句编写中需要注意的问题作详细介绍。在这些where子句中，即使某些列存在索引，但是由于编写了劣质的SQL，系统在运行该SQL语句时也不能使用该索引，而同样使用全表扫描，这就造成了响应速度的极大降低。 1. 操作符优化 (a) IN 操作符 用IN写出来的SQL的优点是比较容易写及清晰易懂，这比较适合现代软件开发的风格。但是用IN的SQL性能总是比较低的，从Oracle执行的步骤来分析用IN的SQL与不用IN的SQL有以下区别： ORACLE试图将其转换成多个表的连接，如果转换不成功则先执行IN里面的子查询，再查询外层的表记录，如果转换成功则直接采用多个表的连接方式查询。由此可见用IN的SQL至少多了一个转换的过程。一般的SQL都可以转换成功，但对于含有分组统计等方面的SQL就不能转换了。 推荐方案：在业务密集的SQL当中尽量不采用IN操作符，用EXISTS 方案代替。 (b) NOT IN操作符 此操作是强列不推荐使用的，因为它不能应用表的索引。 推荐方案：用NOT EXISTS 方案代替 (c) IS NULL 或IS NOT NULL操作（判断字段是否为空） 判断字段是否为空一般是不会应用索引的，因为索引是不索引空值的。不能用null作索引，任何包含null值的列都将不会被包含在索引中。即使索引有多列这样的情况下，只要这些列中有一列含有null，该列就会从索引中排除。也就是说如果某列存在空值，即使对该列建索引也不会提高性能。任何在where子句中使用is null或is not null的语句优化器是不允许使用索引的。 推荐方案：用其它相同功能的操作运算代替，如：a is not null 改为 a&gt;0 或a&gt;’’等。不允许字段为空，而用一个缺省值代替空值，如申请中状态字段不允许为空，缺省为申请。 (d) &gt; 及 &lt; 操作符（大于或小于操作符） 大于或小于操作符一般情况下是不用调整的，因为它有索引就会采用索引查找，但有的情况下可以对它进行优化，如一个表有100万记录，一个数值型字段A，30万记录的A=0，30万记录的A=1，39万记录的A=2，1万记录的A=3。那么执行A&gt;2与A&gt;=3的效果就有很大的区别了，因为A&gt;2时ORACLE会先找出为2的记录索引再进行比较，而A&gt;=3时ORACLE则直接找到=3的记录索引。 (e) LIKE操作符 LIKE操作符可以应用通配符查询，里面的通配符组合可能达到几乎是任意的查询，但是如果用得不好则会产生性能上的问题，如LIKE ‘%5400%’ 这种查询不会引用索引，而LIKE ‘X5400%’则会引用范围索引。 一个实际例子：用YW_YHJBQK表中营业编号后面的户标识号可来查询营业编号 YY_BH LIKE ‘%5400%’ 这个条件会产生全表扫描，如果改成YY_BH LIKE ’X5400%’ OR YY_BH LIKE ’B5400%’ 则会利用YY_BH的索引进行两个范围的查询，性能肯定大大提高。 带通配符(%)的like语句： 同样以上面的例子来看这种情况。目前的需求是这样的，要求在职工表中查询名字中包含cliton的人。可以采用如下的查询SQL语句: select * from employee where last_name like ‘%cliton%’; 这里由于通配符(%)在搜寻词首出现，所以Oracle系统不使用last_name的索引。在很多情况下可能无法避免这种情况，但是一定要心中有底，通配符如此使用会降低查询速度。然而当通配符出现在字符串其他位置时，优化器就能利用索引。在下面的查询中索引得到了使用: select * from employee where last_name like ‘c%’; (f) UNION操作符 UNION在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。实际大部分应用中是不会产生重复的记录，最常见的是过程表与历史表UNION。如：select from gc_dfysunionselect from ls_jg_dfys这个SQL在运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。 推荐方案：采用UNION ALL操作符替代UNION，因为UNION ALL操作只是简单的将两个结果合并后就返回。 select from gc_dfysunion allselect from ls_jg_dfys (g) 联接列 对于有联接的列，即使最后的联接值为一个静态值，优化器是不会使用索引的。我们一起来看一个例子，假定有一个职工表(employee)，对于一个职工的姓和名分成两列存放(FIRST_NAME和LAST_NAME)，现在要查询一个叫比尔.克林顿(Bill Cliton)的职工。 下面是一个采用联接查询的SQL语句： select * from employss where first_name||’’||last_name =’Beill Cliton’; 上面这条语句完全可以查询出是否有Bill Cliton这个员工，但是这里需要注意，系统优化器对基于last_name创建的索引没有使用。当采用下面这种SQL语句的编写，Oracle系统就可以采用基于last_name创建的索引。 * where first_name =’Beill’ and last_name =’Cliton’; (h) Order by语句 ORDER BY语句决定了Oracle如何将返回的查询结果排序。Order by语句对要排序的列没有什么特别的限制，也可以将函数加入列中(象联接或者附加等)。任何在Order by语句的非索引项或者有计算表达式都将降低查询速度。 仔细检查order by语句以找出非索引项或者表达式，它们会降低性能。解决这个问题的办法就是重写order by语句以使用索引，也可以为所使用的列建立另外一个索引，同时应绝对避免在order by子句中使用表达式。 (i) NOT 我们在查询时经常在where子句使用一些逻辑表达式，如大于、小于、等于以及不等于等等，也可以使用and(与)、or(或)以及not(非)。NOT可用来对任何逻辑运算符号取反。下面是一个NOT子句的例子: … where not (status =’VALID’) 如果要使用NOT，则应在取反的短语前面加上括号，并在短语前面加上NOT运算符。NOT运算符包含在另外一个逻辑运算符中，这就是不等于(&lt;&gt;)运算符。换句话说，即使不在查询where子句中显式地加入NOT词，NOT仍在运算符中，见下例: … where status &lt;&gt;’INVALID’; 对这个查询，可以改写为不使用NOT： select * from employee where salary3000; 虽然这两种查询的结果一样，但是第二种查询方案会比第一种查询方案更快些。第二种查询允许Oracle对salary列使用索引，而第一种查询则不能使用索引。 2. SQL书写的影响 (a) 同一功能同一性能不同写法SQL的影响。 如一个SQL在A程序员写的为 Select * from zl_yhjbqk B程序员写的为 Select * from dlyx.zl_yhjbqk（带表所有者的前缀） C程序员写的为 Select * from DLYX.ZLYHJBQK（大写表名） D程序员写的为 Select * from DLYX.ZLYHJBQK（中间多了空格） 以上四个SQL在ORACLE分析整理之后产生的结果及执行的时间是一样的，但是从ORACLE共享内存SGA的原理，可以得出ORACLE对每个SQL 都会对其进行一次分析，并且占用共享内存，如果将SQL的字符串及格式写得完全相同，则ORACLE只会分析一次，共享内存也只会留下一次的分析结果，这不仅可以减少分析SQL的时间，而且可以减少共享内存重复的信息，ORACLE也可以准确统计SQL的执行频率。 (b) WHERE后面的条件顺序影响 WHERE子句后面的条件顺序对大数据量表的查询会产生直接的影响。如：Select from zl_yhjbqk where dy_dj = ‘1KV以下’ and xh_bz=1Select from zl_yhjbqk where xh_bz=1 and dy_dj = ‘1KV以下’以上两个SQL中dy_dj（电压等级）及xh_bz（销户标志）两个字段都没进行索引，所以执行的时候都是全表扫描，第一条SQL的dy_dj = ‘1KV以下’条件在记录集内比率为99%，而xh_bz=1的比率只为0.5%，在进行第一条SQL的时候99%条记录都进行dy_dj及xh_bz的比较，而在进行第二条SQL的时候0.5%条记录都进行dy_dj及xh_bz的比较，以此可以得出第二条SQL的CPU占用率明显比第一条低。 (c) 查询表顺序的影响 在FROM后面的表中的列表顺序会对SQL执行性能影响，在没有索引及ORACLE没有对表进行统计分析的情况下，ORACLE会按表出现的顺序进行链接，由此可见表的顺序不对时会产生十分耗服物器资源的数据交叉。（注：如果对表进行了统计分析，ORACLE会自动先进小表的链接，再进行大表的链接） 3. SQL语句索引的利用 (a) 对条件字段的一些优化 采用函数处理的字段不能利用索引，如： substr(hbs_bh,1,4)=’5400’，优化处理：hbs_bh like ‘5400%’ trunc(sk_rq)=trunc(sysdate)， 优化处理：sk_rq&gt;=trunc(sysdate) and sk_rq&lt;trunc(sysdate+1) 进行了显式或隐式的运算的字段不能进行索引，如：ss_df+20&gt;50，优化处理：ss_df&gt;30 ‘X’ || hbs_bh&gt;’X5400021452’，优化处理：hbs_bh&gt;’5400021542’ sk_rq+5=sysdate，优化处理：sk_rq=sysdate-5 hbs_bh=5401002554，优化处理：hbs_bh=’ 5401002554’，注：此条件对hbs_bh 进行隐式的to_number转换，因为hbs_bh字段是字符型。 条件内包括了多个本表的字段运算时不能进行索引，如： ys_df&gt;cx_df，无法进行优化qc_bh || kh_bh=’5400250000’，优化处理：qc_bh=’5400’ and kh_bh=’250000’ 4. 更多方面SQL优化资料分享 （1） 选择最有效率的表名顺序(只在基于规则的优化器中有效)： ORACLE 的解析器按照从右到左的顺序处理FROM子句中的表名，FROM子句中写在最后的表(基础表 driving table)将被最先处理，在FROM子句中包含多个表的情况下,你必须选择记录条数最少的表作为基础表。如果有3个以上的表连接查询, 那就需要选择交叉表(intersection table)作为基础表, 交叉表是指那个被其他表所引用的表. （2） WHERE子句中的连接顺序： ORACLE采用自下而上的顺序解析WHERE子句,根据这个原理,表之间的连接必须写在其他WHERE条件之前, 那些可以过滤掉最大数量记录的条件必须写在WHERE子句的末尾. （3） SELECT子句中避免使用 ‘ * ‘： ORACLE在解析的过程中, 会将’*’ 依次转换成所有的列名, 这个工作是通过查询数据字典完成的, 这意味着将耗费更多的时间。 （4） 减少访问数据库的次数： ORACLE在内部执行了许多工作: 解析SQL语句, 估算索引的利用率, 绑定变量 , 读数据块等。 （5） 在SQLPlus , SQLForms和Pro*C中重新设置ARRAYSIZE参数, 可以增加每次数据库访问的检索数据量 ,建议值为200。 （6） 使用DECODE函数来减少处理时间： 使用DECODE函数可以避免重复扫描相同记录或重复连接相同的表. （7） 整合简单,无关联的数据库访问： 如果你有几个简单的数据库查询语句,你可以把它们整合到一个查询中(即使它们之间没有关系) 。 （8） 删除重复记录： 最高效的删除重复记录方法 ( 因为使用了ROWID)例子：DELETE FROM EMP E WHERE E.ROWID &gt; (SELECT MIN(X.ROWID) FROM EMP X WHERE X.EMP_NO = E.EMP_NO)。 （9） 用TRUNCATE替代DELETE： 当删除表中的记录时,在通常情况下, 回滚段(rollback segments ) 用来存放可以被恢复的信息. 如果你没有COMMIT事务,ORACLE会将数据恢复到删除之前的状态(准确地说是恢复到执行删除命令之前的状况) 而当运用TRUNCATE时, 回滚段不再存放任何可被恢复的信息.当命令运行后,数据不能被恢复.因此很少的资源被调用,执行时间也会很短. (译者按: TRUNCATE只在删除全表适用,TRUNCATE是DDL不是DML) 。 （10） 尽量多使用COMMIT： 只要有可能,在程序中尽量多使用COMMIT, 这样程序的性能得到提高,需求也会因为COMMIT所释放的资源而减少，COMMIT所释放的资源:a. 回滚段上用于恢复数据的信息.b. 被程序语句获得的锁c. redo log buffer 中的空间d. ORACLE为管理上述3种资源中的内部花费 （11） 用Where子句替换HAVING子句： 避免使用HAVING子句, HAVING 只会在检索出所有记录之后才对结果集进行过滤. 这个处理需要排序,总计等操作. 如果能通过WHERE子句限制记录的数目,那就能减少这方面的开销. (非oracle中)on、where、having这三个都可以加条件的子句中，on是最先执行，where次之，having最后，因为on是先把不符合条件的记录过滤后才进行统计，它就可以减少中间运算要处理的数据，按理说应该速度是最快的，where也应该比having快点的，因为它过滤数据后才进行sum，在两个表联接时才用on的，所以在一个表的时候，就剩下where跟having比较了。在这单表查询统计的情况下，如果要过滤的条件没有涉及到要计算字段，那它们的结果是一样的，只是where可以使用rushmore技术，而having就不能，在速度上后者要慢如果要涉及到计算的字 段，就表示在没计算之前，这个字段的值是不确定的，根据上篇写的工作流程，where的作用时间是在计算之前就完成的，而having就是在计算后才起作 用的，所以在这种情况下，两者的结果会不同。在多表联接查询时，on比where更早起作用。系统首先根据各个表之间的联接条件，把多个表合成一个临时表 后，再由where进行过滤，然后再计算，计算完后再由having进行过滤。由此可见，要想过滤条件起到正确的作用，首先要明白这个条件应该在什么时候起作用，然后再决定放在那里。 （12） 减少对表的查询： 在含有子查询的SQL语句中,要特别注意减少对表的查询.例子：SELECT TAB_NAME FROM TABLES WHERE (TAB_NAME,DB_VER) = ( SELECT TAB_NAME,DB_VER FROM TAB_COLUMNS WHERE VERSION = 604) （13） 通过内部函数提高SQL效率： 复杂的SQL往往牺牲了执行效率. 能够掌握上面的运用函数解决问题的方法在实际工作中是非常有意义的。 （14） 使用表的别名(Alias)： 当在SQL语句中连接多个表时, 请使用表的别名并把别名前缀于每个Column上.这样一来,就可以减少解析的时间并减少那些由Column歧义引起的语法错误。 （15） 用EXISTS替代IN、用NOT EXISTS替代NOT IN： 在许多基于基础表的查询中,为了满足一个条件,往往需要对另一个表进行联接.在这种情况下, 使用EXISTS(或NOT EXISTS)通常将提高查询的效率. 在子查询中,NOT IN子句将执行一个内部的排序和合并. 无论在哪种情况下,NOT IN都是最低效的 (因为它对子查询中的表执行了一个全表遍历). 为了避免使用NOT IN ,我们可以把它改写成外连接(Outer Joins)或NOT EXISTS。例子：（高效）SELECT FROM EMP (基础表) WHERE EMPNO &gt; 0 AND EXISTS (SELECT ‘X’ FROM DEPT WHERE DEPT.DEPTNO = EMP.DEPTNO AND LOC = ‘MELB’)(低效)SELECT FROM EMP (基础表) WHERE EMPNO &gt; 0 AND DEPTNO IN(SELECT DEPTNO FROM DEPT WHERE LOC = ‘MELB’) （16） 识别’低效执行’的SQL语句： 虽然目前各种关于SQL优化的图形化工具层出不穷,但是写出自己的SQL工具来解决问题始终是一个最好的方法：SELECT EXECUTIONS , DISK_READS, BUFFER_GETS,ROUND((BUFFER_GETS-DISK_READS)/BUFFER_GETS,2) Hit_radio,ROUND(DISK_READS/EXECUTIONS,2) Reads_per_run,SQL_TEXTFROM V$SQLAREAWHERE EXECUTIONS&gt;0AND BUFFER_GETS &gt; 0AND (BUFFER_GETS-DISK_READS)/BUFFER_GETS &lt; 0.8ORDER BY 4 DESC; （17） 用索引提高效率： 索引是表的一个概念部分,用来提高检索数据的效率，ORACLE使用了一个复杂的自平衡B-tree结构. 通常,通过索引查询数据比全表扫描要快. 当ORACLE找出执行查询和Update语句的最佳路径时, ORACLE优化器将使用索引. 同样在联结多个表时使用索引也可以提高效率. 另一个使用索引的好处是,它提供了主键(primary key)的唯一性验证.。那些LONG或LONG RAW数据类型, 你可以索引几乎所有的列. 通常, 在大型表中使用索引特别有效. 当然,你也会发现, 在扫描小表时,使用索引同样能提高效率. 虽然使用索引能得到查询效率的提高,但是我们也必须注意到它的代价. 索引需要空间来存储,也需要定期维护, 每当有记录在表中增减或索引列被修改时, 索引本身也会被修改. 这意味着每条记录的INSERT , DELETE , UPDATE将为此多付出4 , 5 次的磁盘I/O . 因为索引需要额外的存储空间和处理,那些不必要的索引反而会使查询反应时间变慢.。定期的重构索引是有必要的：ALTER INDEX REBUILD （18） 用EXISTS替换DISTINCT： 当提交一个包含一对多表信息(比如部门表和雇员表)的查询时,避免在SELECT子句中使用DISTINCT. 一般可以考虑用EXIST替换, EXISTS 使查询更为迅速,因为RDBMS核心模块将在子查询的条件一旦满足后,立刻返回结果. 例子：(低效):SELECT DISTINCT DEPT_NO,DEPT_NAME FROM DEPT D , EMP E WHERE D.DEPT_NO = E.DEPT_NO(高效):SELECT DEPT_NO,DEPT_NAME FROM DEPT D WHERE EXISTS ( SELECT ‘X’ FROM EMP E WHERE E.DEPT_NO = D.DEPT_NO); （19） sql语句用大写的；因为oracle总是先解析sql语句，把小写的字母转换成大写的再执行。 （20） 在java代码中尽量少用连接符“＋”连接字符串！ （21） 避免在索引列上使用NOT，通常我们要避免在索引列上使用NOT, NOT会产生在和在索引列上使用函数相同的影响. 当ORACLE”遇到”NOT,他就会停止使用索引转而执行全表扫描。 （22） 避免在索引列上使用计算 WHERE子句中，如果索引列是函数的一部分．优化器将不使用索引而使用全表扫描．举例:低效：SELECT … FROM DEPT WHERE SAL * 12 &gt; 25000;高效:SELECT … FROM DEPT WHERE SAL &gt; 25000/12; （23） 用&gt;=替代&gt;高效:SELECT FROM EMP WHERE DEPTNO &gt;=4低效:SELECT FROM EMP WHERE DEPTNO &gt;3两者的区别在于, 前者DBMS将直接跳到第一个DEPT等于4的记录而后者将首先定位到DEPTNO=3的记录并且向前扫描到第一个DEPT大于3的记录。 （24） 用UNION替换OR (适用于索引列) 通常情况下, 用UNION替换WHERE子句中的OR将会起到较好的效果. 对索引列使用OR将造成全表扫描. 注意, 以上规则只针对多个索引列有效. 如果有column没有被索引, 查询效率可能会因为你没有选择OR而降低. 在下面的例子中, LOC_ID 和REGION上都建有索引.高效:SELECT LOC_ID , LOC_DESC , REGIONFROM LOCATIONWHERE LOC_ID = 10UNIONSELECT LOC_ID , LOC_DESC , REGIONFROM LOCATIONWHERE REGION = “MELBOURNE”低效:SELECT LOC_ID , LOC_DESC , REGIONFROM LOCATIONWHERE LOC_ID = 10 OR REGION = “MELBOURNE”如果你坚持要用OR, 那就需要返回记录最少的索引列写在最前面. （25） 用IN来替换OR 这是一条简单易记的规则，但是实际的执行效果还须检验，在ORACLE8i下，两者的执行路径似乎是相同的．低效:SELECT…. FROM LOCATION WHERE LOC_ID = 10 OR LOC_ID = 20 OR LOC_ID = 30高效SELECT… FROM LOCATION WHERE LOC_IN IN (10,20,30); （26） 避免在索引列上使用IS NULL和IS NOT NULL 避免在索引中使用任何可以为空的列，ORACLE将无法使用该索引．对于单列索引，如果列包含空值，索引中将不存在此记录. 对于复合索引，如果每个列都为空，索引中同样不存在此记录. 如果至少有一个列不为空，则记录存在于索引中．举例: 如果唯一性索引建立在表的A列和B列上, 并且表中存在一条记录的A,B值为(123,null) , ORACLE将不接受下一条具有相同A,B值（123,null）的记录(插入). 然而如果所有的索引列都为空，ORACLE将认为整个键值为空而空不等于空. 因此你可以插入1000 条具有相同键值的记录,当然它们都是空! 因为空值不存在于索引列中,所以WHERE子句中对索引列进行空值比较将使ORACLE停用该索引.低效: (索引失效)SELECT … FROM DEPARTMENT WHERE DEPT_CODE IS NOT NULL;高效: (索引有效)SELECT … FROM DEPARTMENT WHERE DEPT_CODE &gt;=0; （27） 总是使用索引的第一个列： 如果索引是建立在多个列上, 只有在它的第一个列(leading column)被where子句引用时,优化器才会选择使用该索引. 这也是一条简单而重要的规则，当仅引用索引的第二个列时,优化器使用了全表扫描而忽略了索引。 （28） 用UNION-ALL 替换UNION ( 如果有可能的话)： 当SQL 语句需要UNION两个查询结果集合时,这两个结果集合会以UNION-ALL的方式被合并, 然后在输出最终结果前进行排序. 如果用UNION ALL替代UNION, 这样排序就不是必要了. 效率就会因此得到提高. 需要注意的是，UNION ALL 将重复输出两个结果集合中相同记录. 因此各位还是要从业务需求分析使用UNION ALL的可行性. UNION 将对结果集合排序,这个操作会使用到SORT_AREA_SIZE这块内存. 对于这块内存的优化也是相当重要的. 下面的SQL可以用来查询排序的消耗量低效：SELECT ACCT_NUM, BALANCE_AMTFROM DEBIT_TRANSACTIONSWHERE TRAN_DATE = ‘31-DEC-95’UNIONSELECT ACCT_NUM, BALANCE_AMTFROM DEBIT_TRANSACTIONSWHERE TRAN_DATE = ‘31-DEC-95’高效:SELECT ACCT_NUM, BALANCE_AMTFROM DEBIT_TRANSACTIONSWHERE TRAN_DATE = ‘31-DEC-95’UNION ALLSELECT ACCT_NUM, BALANCE_AMTFROM DEBIT_TRANSACTIONSWHERE TRAN_DATE = ‘31-DEC-95’ （29） 用WHERE替代ORDER BY： ORDER BY 子句只在两种严格的条件下使用索引.ORDER BY中所有的列必须包含在相同的索引中并保持在索引中的排列顺序.ORDER BY中所有的列必须定义为非空.WHERE子句使用的索引和ORDER BY子句中所使用的索引不能并列.例如:表DEPT包含以下列:DEPT_CODE PK NOT NULLDEPT_DESC NOT NULLDEPT_TYPE NULL低效: (索引不被使用)SELECT DEPT_CODE FROM DEPT ORDER BY DEPT_TYPE高效: (使用索引)SELECT DEPT_CODE FROM DEPT WHERE DEPT_TYPE &gt; 0 （30） 避免改变索引列的类型: 当比较不同数据类型的数据时, ORACLE自动对列进行简单的类型转换.假设 EMPNO是一个数值类型的索引列.SELECT … FROM EMP WHERE EMPNO = ‘123’实际上,经过ORACLE类型转换, 语句转化为:SELECT … FROM EMP WHERE EMPNO = TO_NUMBER(‘123’)幸运的是,类型转换没有发生在索引列上,索引的用途没有被改变.现在,假设EMP_TYPE是一个字符类型的索引列.SELECT … FROM EMP WHERE EMP_TYPE = 123这个语句被ORACLE转换为:SELECT … FROM EMP WHERE TO_NUMBER(EMP_TYPE)=123因为内部发生的类型转换, 这个索引将不会被用到! 为了避免ORACLE对你的SQL进行隐式的类型转换, 最好把类型转换用显式表现出来. 注意当字符和数值比较时, ORACLE会优先转换数值类型到字符类型。 分析select emp_name form employee where salary &gt; 3000 在此语句中若salary是Float类型的，则优化器对其进行优化为Convert(float,3000)，因为3000是个整数，我们应在编程时使用3000.0而不要等运行时让DBMS进行转化。同样字符和整型数据的转换。 （31） 需要当心的WHERE子句: 某些SELECT 语句中的WHERE子句不使用索引. 这里有一些例子.在下面的例子里, (1)‘!=’ 将不使用索引. 记住, 索引只能告诉你什么存在于表中, 而不能告诉你什么不存在于表中. (2) ‘ ¦ ¦’是字符连接函数. 就象其他函数那样, 停用了索引. (3) ‘+’是数学函数. 就象其他数学函数那样, 停用了索引. (4)相同的索引列不能互相比较,这将会启用全表扫描. （32） a. 如果检索数据量超过30%的表中记录数.使用索引将没有显著的效率提高. b. 在特定情况下, 使用索引也许会比全表扫描慢, 但这是同一个数量级上的区别. 而通常情况下,使用索引比全表扫描要块几倍乃至几千倍! （33） 避免使用耗费资源的操作： 带有DISTINCT,UNION,MINUS,INTERSECT,ORDER BY的SQL语句会启动SQL引擎执行耗费资源的排序(SORT)功能. DISTINCT需要一次排序操作, 而其他的至少需要执行两次排序. 通常, 带有UNION, MINUS , INTERSECT的SQL语句都可以用其他方式重写. 如果你的数据库的SORT_AREA_SIZE调配得好, 使用UNION , MINUS, INTERSECT也是可以考虑的, 毕竟它们的可读性很强。 （34） 优化GROUP BY： 提高GROUP BY 语句的效率, 可以通过将不需要的记录在GROUP BY 之前过滤掉.下面两个查询返回相同结果但第二个明显就快了许多.低效:SELECT JOB , AVG(SAL)FROM EMPGROUP by JOBHAVING JOB = ‘PRESIDENT’OR JOB = ‘MANAGER’高效:SELECT JOB , AVG(SAL)FROM EMPWHERE JOB = ‘PRESIDENT’OR JOB = ‘MANAGER’GROUP by JOB 转载地址]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM笔记整理]]></title>
    <url>%2F20180120-jvm%2F</url>
    <content type="text"><![CDATA[谈到JVM大家可能会想到JVM调优.最近在学习JVM之类的知识,整理了一些相关的笔记. 1.为什么要调优?说到调优其实不是每个系统都需要调优,其实在默认情况下,JVM默认的参数已经是最优的了,大部分的应用无需再修改任何的JVM参数,对于有钱的公司一般最有效和最方便的方法就是加机器,加机器,还是加机器.只有当加机器还是无法解决问题时,这时候就可以考虑JVM调优了. 2. 如何知道性能需要提升调优当然需要知道是哪里出了问题,现在有很多的开源的监控工具可以协助我们找出问题根源,例如:点评的CAT,还有Pinpoint(一个分布式事务跟踪系统的平台),都很好用的. 当问题出现了,一般从四个方面来分析: cpu memory io network 关于cpu,可以使用top命令,观察导致cpu标高进程或者线程,然后再配合jstack工具找出具体的元婴 basic1234top (找到cpu高的线程PID) 或者 使用 top -H -p 16326 找到java进程包含的线程PID (16326是通过jps命令查到java进程的ID)jstack PID &gt; a.txt (输出到文件中)printf &quot;%x \n&quot; PID //3fc6 (转换10进制为16进度)在a.txt文件中查询十六进制3fc6,对应的线程栈信息,锁定问题的根源. 查看机器的IO可以使用命令iostat观察 3. JDK自带的监控工具basic123456jmap -heap pid 堆使用情况jstat -gcutil pid 1000 5 jstack 线程pidjvisualvmjconsolejinfo 查看JVM启动参数 JDK官方文档: https://docs.oracle.com/javase/8/docs/technotes/tools/windows/toc.html 4. JVM参数 -X 开头的参数都是非标准的参数（不是所有的JVM都实现了） java -X可查看JVM参数的含义 常用参数: starting12345678-Xms :设置Java堆栈的初始化大小(starting)-Xmx :设置最大的java堆大小(max)-Xmn :设置Young区大小(new)-Xss :设置java线程堆栈大小-XX:PermSize and MaxPermSize :设置持久带的大小-XX:NewRatio :设置年轻代和老年代的比值-XX:NewSize :设置年轻代的大小-XX:SurvivorRation=n :设置年轻代中E去与俩个S去的比值 //-XX:SurvivorRatio=8 对象分配eden 8:1:1 官方文档: http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html 5. GC日志 输出日志 -verbose:gc :记录GC运行以及运行时间,一般用来查看GC是否有瓶颈-XX:+PrintGCDetails :记录GC运行时的详细数据信息，包括新生占用的内存大小及消耗时间-XX:-PrintGCTimeStamps :打印收集的时间戳-XX:+UseParallelGC :使用并行垃圾收集器-XX:-UseConcMarkSweepGC :使用并发标志扫描收集器-XX:-UseSerialGC :使用串行垃圾收集器-Xloggc:/data/gc/gc.log :设置GC记录的文件 日志文件控制 -XX:-UseGCLogFileRotation //启用GC日志文件的自动转储-XX:GCLogFileSize=8K //控制GC日志文件的大小 YoungGC对照图 FullGC对照图]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.3更换内核安装锐速破解版]]></title>
    <url>%2F20171207-Centos-install-serverspeeder%2F</url>
    <content type="text"><![CDATA[centos更换内核安装serverspeeder锐速, 加速后的访问速度提高很多. CentO S7.3的内核3.10.0-514.16.1.el7.x86_64暂不支持安装锐速，故更换为3.10.0-229.1.2.el7.x86_64 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-3.10.0-229.1.2.el7.x86_64.rpm --force 查看内核是否安装成功 1rpm -qa | grep kernel 如果显示里面有这个内核就对了。 重启，查看内核是否更换成功 12rebootuname -r 更换完内核后开始安装锐速破解版 1wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 查看状态 1service serverSpeeder status 查看实时信息 1service serverSpeeder stats 锐速服务是开机启动的，所以不用进行设置 锐速破解版卸载方法 1chattr -i /serverspeeder/etc/apx* &amp;&amp; /serverspeeder/bin/serverSpeeder.sh uninstall -f]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vpn</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己专属的vpn——Centos搭建vpn的几种办法]]></title>
    <url>%2F20171203-select-vpn%2F</url>
    <content type="text"><![CDATA[方法一：搭建shadowsocks+serverspeeder（特别推荐）shadowsocks服务端安装参考官方Shadowsocks使用说明： CentOS: 12yum install python-setuptools &amp;&amp; easy_install pip pip install shadowsocks Debian / Ubuntu: 12apt-get install python-pip pip install shadowsocks 配置参考Configuration via Config File 修改配置文件/etc/shadowsocks.json，如果没有则新建。内容如下： 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:8388, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false &#125; 或（多个SS账号） 123456789101112&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;port_password&quot;:&#123; &quot;8381&quot;:&quot;xxxxxxx&quot;, &quot;8382&quot;:&quot;xxxxxxx&quot;, &quot;8383&quot;:&quot;xxxxxxx&quot;, &quot;8384&quot;:&quot;xxxxxxx&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false &#125; 配置说明： 字段 说明 server ss服务监听地址 server_port ss服务监听端口 local_address 本地的监听地址 local_port 本地的监听端口 password 密码 timeout 超时时间，单位秒 method 加密方法，默认是aes-256-cfb fast_open 使用TCP_FASTOPEN, true / false workers workers数，只支持Unix/Linux系统 启动：前台启动 1ssserver -c /etc/shadowsocks.json 后台启动与停止 12ssserver -c /etc/shadowsocks.json -d start ssserver -c /etc/shadowsocks.json -d stop 如需开机启动修改/etc/rc.local，加入以下内容 1ssserver -c /etc/shadowsocks.json -d start 日志shadowsocks的日志保存在 /var/log/shadowsocks.log shadowsocks客户端安装下载地址： 1234567891011121314151617Windows https://github.com/shadowsocks/shadowsocks-windows/releases Mac OS X https://github.com/shadowsocks/ShadowsocksX-NG/releases Linux https://github.com/shadowsocks/shadowsocks-qt5/wiki/Installation https://github.com/shadowsocks/shadowsocks-qt5/releases IOS https://itunes.apple.com/app/apple-store/id1070901416?pt=2305194&amp;ct=shadowsocks.org&amp;mt=8 https://github.com/shadowsocks/shadowsocks-iOS/releasesAndroid https://play.google.com/store/apps/details?id=com.github.shadowsocks https://github.com/shadowsocks/shadowsocks-android/releases serverspeeder加速安装注意：serverspeeder加速是可选的，如果你使用vpn测速发现很慢，可以安装试试。 加速前 加速后 下行速度瞬间提升，是不是觉得有点小激动？一键安装serverspeeder注：参考serverspeeder锐速一键破解安装版 1wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 如果报内核不支持，可以更换系统内核 123456789101112CentO S7.3的内核3.10.0-514.16.1.el7.x86_64暂不支持安装锐速，故更换为3.10.0-229.1.2.el7.x86_64下载内核安装包 rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-3.10.0-229.1.2.el7.x86_64.rpm --force查看内核是否安装成功rpm -qa | grep kernel重启 reboot 查看内核版本是否替换成功uname -r 更换完内核后开始安装锐速破解版 1wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 查看状态 1service serverSpeeder status 查看实时信息 1service serverSpeeder stats 如果系统内核已更新，再次执行一键安装serverspeeder方法即可。至此serverspeeder安装完毕，快去试试速度是不提升了。 卸载serverspeeder的方法 1chattr -i /serverspeeder/etc/apx* &amp;&amp; /serverspeeder/bin/serverSpeeder.sh uninstall -f 方法二：搭建l2tp vpn（推荐）参考DearTanker’s Blog的一键安装方法： 123wget --no-check-certificate https://raw.githubusercontent.com/teddysun/across/master/l2tp.sh chmod +x l2tp.sh ./l2tp.sh 基本上，按交互式命令的提示按回车或者自定义自己的选择即可，然后再次验证ipsec（L2TP）并重启相关服务，否则提示服务器无响应 123service ipsec restart service xl2tpd restart ipsec verify 如果需要修改或者增加账号密码，可以修改/etc/ppp/chap-secrets 1账户 l2tpd 密码 * 测试成功使用的客户端有，win7+win10自带vpn客户端，andriod自带vpn客户端。 测试成功使用的网络环境有，电信+联通宽带，移动4g无法连接成功，如果哪位朋友在移动网络下有成功的经验，麻烦分享一下。 不使用vpn时，使用speedtest的测速结果： 下面是成功连接vpn后，使用speedtest的测速结果，虽然不是非常快，但基本够用。 方法三：搭建openvpn（不建议）vultr面板提供一键安装openvpn的办法，方法是在新建一个vps实例时选择默认安装一个应用程序： 具体安装办法参考官方的一键安装openvpn说明。openvpn是使用操作系统的登录账号登录的，所以搭建完openvpn后，你可以参考linux新建用户的办法新建一个用户及修改用户密码，然后使用opevpn提供的客户端或者网页（一般情况下是 https://your_vps_ip:943/ ）登录即可。 之所以不建议使用openvpn，原因是测速发现比较慢，下载带宽不到1M，而且openvpn貌似不支持移动端登录。 方法四：搭建pptpd vpn（不建议）注：之所以不建议使用pptpd，一方面是pptpd经常被墙，二是容易出问题。 安装1yum install ppp iptables pptpd 配置编辑/etc/pptpd.conf，搜索localip，去掉下面字段前面的#，然后保存退出 12localip 192.168.0.1 remoteip 192.168.0.234-238,192.168.0.245 注意，pptpd默认支持最大100个连接，每个remoteip分配一个连接，如果remoteip数不够100个，那么默认连接数就会变成remoteip数。如果默认连接数不够的话，就会出现自动断开的情况，比如手机上的vpn连上了，PC端的vpn就会断开。 编辑options.pptpd，搜索ms-dns，去掉搜索到的两行ms-dns前面的#，并修改为下面的字段 12ms-dns 8.8.8.8 ms-dns 8.8.4.4 编辑/etc/ppp/chap-secrets设置VPN的帐号密码，注意，用户名与密码是区分大小写的 1用户名 pptpd 密码 * 编辑/etc/sysctl.conf，修改内核参数，在末尾添加下面的代码，使内核支持转发 1net.ipv4.ip_forward=1 运行下面的命令使内核修改生效 1sysctl -p 添加下面的iptables转发规则（直接在SSH运行下面命令即可） 1iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE 添加转发规则后重启就会失效，Centos 6系统可以使用service iptables save保存配置，而Centos7则可以修改/etc/rc.d/rc.local保存上面的命令，这样开机会自动执行上面的命令。 启动用下面的命令使pptpd开机自动启动 1chkconfig pptpd on 启动pptpd 1service pptpd start 使用使用你的vpn客户端连接即可，如果配置没问题的话，就可以连接成功。测试成功连接的vpn客户端有，win7+win10自带vpn客户端，andriod自带vpn客户端。但只在宽带网络上连接成功，4g网络连接不成功。 排错如果你的vpn连接不成功，有可能是iptable防火墙的问题，你可以使用下面命令 123iptables -A INPUT -p tcp --dport 1723 -j ACCEPT 或者 iptables -F 然后在你的其他电脑使用telnet your_ip 1723测试是否连通。 如果你遇到访问网站偶尔连接上又断的问题，可能是MTU太大导致，可以 1234执行 iptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356 或者修改/etc/ppp/options.pptpd，在文件最后添加 mtu 1356 更多问题，可以打开pptpd的debug日志，根据debug日志的输出上网搜索一步步解决修改/etc/ppp/options.pptpd取消下面的注释 123debug dump logfile /var/log/pptpd.log（没有则手工修改） 转载地址: http://blog.noahsun.top/2018/04/07/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E4%B8%93%E5%B1%9E%E7%9A%84vpn%E2%80%94%E2%80%94Centos%E6%90%AD%E5%BB%BAvpn%E7%9A%84%E5%87%A0%E7%A7%8D%E5%8A%9E%E6%B3%95/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手搭建免费的VPN服务器]]></title>
    <url>%2F20171202-vpn%2F</url>
    <content type="text"><![CDATA[由于平常查询资料经常需要翻墙google之类的网站,之前一直买的第三方的VPN账号,但是经常遇到不稳定或者访问比较慢的问题.后来实在是无法忍受了,决定搭建一个属于自己的VPN.具体的步骤总结一下,也希望能帮到需要的朋友. 选择服务器首先需要购买服务器,购买前我也看了好几家的云服务器,价钱都差不多,最低配置的价格都是5$ 亚马逊的LigntSail, 需要绑定信用卡,扣了我12块钱预付费,收到短信才知道直接偷偷扣钱了(真心无语),随后主机也弄好了,ssh秘钥也生成了,但是就是本地ssh不能访问服务器,换了其他的地区和操作系统,仍然不能访问,索性放弃了,估计是QWS的bug… digitalocean 可以支持信用卡和PayPal支付,既然可以不用绑定信用卡当然选择用PayPal支付,随后注册了一个PayPal账户,绑定一下银行卡,就可以直接支付服务器的费用了. vultr 这个是我最终的选择,重要一点是可以支持支付宝付款,很方便,机房也挺多的,我选的是日本的服务器,也建议选择这个区域的,网速还可以. 安装Shadowsocks服务端使用root用户登录你的服务器,运行一下命令: 123wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.shchmod +x shadowsocksR.sh./shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log 安装完成后，脚本提示如下： 12345678910Congratulations, ShadowsocksR server install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Protocol :your_protocolYour obfs :your_obfsYour Encryption Method:your_encryption_methodWelcome to visit:https://shadowsocks.be/9.htmlEnjoy it! 默认配置：服务器端口：自己设定（如不设定，默认为 8989）密码：自己设定（如不设定，默认为 teddysun.com）加密方式：自己设定（如不设定，默认为 aes-256-cfb）协议（Protocol）：自己设定（如不设定，默认为 origin）混淆（obfs）：自己设定（如不设定，默认为 plain） 卸载方法：使用 root 用户登录，运行命令：./shadowsocksR.sh uninstall 安装完成后即已后台启动 ShadowsocksR ，运行：/etc/init.d/shadowsocks status可以查看 ShadowsocksR 进程是否已经启动。本脚本安装完成后，已将 ShadowsocksR 自动加入开机自启动。 使用命令：启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status 配置文件路径：/etc/shadowsocks.json日志文件路径：/var/log/shadowsocks.log代码安装目录：/usr/local/shadowsocks 客户端安装服务端安装完之后,安装客户端进行连接 https://github.com/iMeiji/shadowsocks_install/releases/tag/0.13 连接成功就可以翻墙了,很简单的. 注意事项：本脚本没有对防火墙（IPv4 是 iptables，IPv6 是 ip6tables）进行任何设置。因此，在安装完毕，如果你发现连接不上，可以尝试更改防火墙设置或关闭防火墙。 参考连接: https://github.com/iMeiji/shadowsocks_install/wiki/shadowsocksR-%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己专属的vpn——选择一个vps]]></title>
    <url>%2F20171201-vps%2F</url>
    <content type="text"><![CDATA[前言作为一名技术人员，经常需要访问一些墙外的网站，你懂的。本来想直接购买个vpn，但又怕不太安全。看到网上有很多自己搭建vpn的教程，于是自己折腾了一下，终于成功搭建，随便与大家分享一些经验。 选择一个vps搭建vpn的第一步是，你必须有一台公网能访问的vps，而且该vps要在墙外，简单来说就是你必须要有一台能访问墙外的电脑。如何选择一个适合自己的vps呢？我自己认为主要有价格、稳定性和带宽等方面的因素，当然易操作性、良好的售后服务等也比较重要。 国外常见的vps有Linode、Vultr、SugarHosts、bandwagonhost等。 看了网上的一些评测，如《Vultr超高性价比VPS评测使用教程》，以及Vultr每月$2.5的费用，觉得可以入手，因为网上购买一个vpn也差不多这个费用。 注册账号首先，登陆Vultr官网，点击Create Account注册即可，之前官方搞活动，暑假注册使用免费送$20，现在好像没有什么优惠了，期待下次还有活动。现在最新优惠是每月$2.5的实例，不过经常没货。 注册并不会收费，只有创建了实例才会收费，如果觉得不好用可以直接删除自己创建的实例，这样就不会收费了。注意，如果实例关机但没有把实例删除，还是会收费的，毕竟该实例占用了公网IP和硬盘空间。 可喜的是，现在vultr已经支持支付宝付款了。 选择机房如何选择一个好的机房，当然是自己测过才放心。这里推荐使用PingInfoView来测试。以下是vultr的机房： 列表如下，贴到PingInfoView上即可。 123456789101112131415sgp-ping.vultr.comhnd-jp-ping.vultr.comsyd-au-ping.vultr.comfra-de-ping.vultr.comams-nl-ping.vultr.comlon-gb-ping.vultr.compar-fr-ping.vultr.comwa-us-ping.vultr.comsjo-ca-us-ping.vultr.comlax-ca-us-ping.vultr.comil-us-ping.vultr.comnj-us-ping.vultr.comtx-us-ping.vultr.comga-us-ping.vultr.comfl-us-ping.vultr.com linux/mac用户可以使用我写的脚本vultr_test来测试。 以下是我的测试结果：选择的原则是，ping值最小，丢包率最小。比如我的测试结果中，较不错的是sjo、wa和lax，都是美国机房。 部署实例点击Servers，然后点右边的+号依次选择自己测试比较好的机房，选择操作系统版本，选择方案（$2.5的低配方案搭vpn已经够用，另外还可以自己搭个博客什么的），然后部署就搞定了。 如果vps使用linux操作系统，可以使用ssh的客户端登录，比如SecureCRT，这里就不再详细说明。 转载地址: http://blog.noahsun.top/2018/04/07/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E4%B8%93%E5%B1%9E%E7%9A%84vpn%E2%80%94%E2%80%94%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AAvps/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vps</tag>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令]]></title>
    <url>%2F20171129-git-operators%2F</url>
    <content type="text"><![CDATA[理解这些指令，觉得最重要的是理解Git的内部原理，比如Git的分布式版本控制，分清楚工作区、暂存区、版本库，还有就是理解Git跟踪并管理的是修改，而非文件。 第一步是要获得一个GIT仓库有两种获得GIT仓库的方法，一是在需要用GIT管理的项目的根目录执行： 1git init 执行后可以看到，仅仅在项目目录多出了一个.git目录，关于版本等的所有信息都在这个目录里面。 另一种方式是克隆远程目录，由于是将远程服务器上的仓库完全镜像一份至本地，而不是取某一个特定版本，所以用clone而不是checkout： 1git clone &lt;url&gt; 设置12$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot; ssh -v git@github.com //检验下Git安装是否正确，显示hi github用户名！You’ve successfully authenticated 说明Git安装正确 提交git tracked的是修改，而不是文件 1234#将“当前修改”移动到暂存区(stage)$ git add somfile.txt#将暂存区修改提交$ git commit -m "Add somfile.txt." 状态12$ git status$ git diff 回退12345678910111213# 放弃工作区修改$ git checkout -- file.name$ git checkout -- .# 取消commit(比如需要重写commit信息)$ git reset --soft HEAD# 取消commit、add(重新提交代码和commit)$ git reset HEAD$ git reset --mixed HEAD# 取消commit、add、工作区修改(需要完全重置)$ git reset --hard HEAD 记录12$ git reflog$ git log 删除123$ rm file.name$ git rm file.name$ git commit -m "Del" Git 远程分支管理1234567891011121314git pull # 抓取远程仓库所有分支更新并合并到本地git pull --no-ff # 抓取远程仓库所有分支更新并合并到本地，不要快进合并git fetch origin # 抓取远程仓库更新git merge origin/master # 将远程主分支合并到本地当前分支git checkout --track origin/branch # 跟踪某个远程分支创建相应的本地分支git checkout -b &lt;local_branch&gt; origin/&lt;remote_branch&gt; # 基于远程分支创建本地分支，功能同上 git push # push所有分支git push origin master # 将本地主分支推到远程主分支# 第一次推送，-u(--set-upstream)指定默认上游git push -u origin master # 将本地主分支推到远程(如无远程主分支则创建，用于初始化远程仓库)git push origin &lt;local_branch&gt; # 创建远程分支， origin是远程仓库名git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; # 创建远程分支git push origin :&lt;remote_branch&gt; #先删除本地分支(git branch -d &lt;branch&gt;)，然后再push删除远程分支 Git 远程仓库管理12345git remote -v # 查看远程服务器地址和仓库名称git remote show origin # 查看远程服务器仓库状态git remote add origin git@github.com:whuhacker/Unblock-Youku-Firefox.git # 添加远程仓库地址git remote set-url origin git@github.com:whuhacker/Unblock-Youku-Firefox.git # 设置远程仓库地址(用于修改远程仓库地址)git remote rm &lt;repository&gt; # 删除远程仓库 克隆12$ git clone https://github.com/Yikun/yikun.github.com.git path$ git clone git@github.com:Yikun/yikun.github.com.git path 分支操作 12345678910111213141516# 查看当前分支$ git branch# 创建分支$ git branch dev# 切换分支$ git checkout dev# 创建并checkout分支$ git checkout -b dev# 合并分支$ git merge dev# 删除分支$ git branch -d dev 标签12$ git tag 0.1.1$ git push origin --tags 目前使用git主要流程就是先在本地clone一个git仓库，然后设置一下看git是否连接成功，接着git remote add origin git@github.com:michaelliao/learngit.git，添加要连接的远程仓库地址，进行push（将本地主分支推到远程主分支）还是pull（抓取远程仓库所有分支更新并合并到本地）操作即可。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[良好的个人作息时间表]]></title>
    <url>%2F20171129-zuoxi%2F</url>
    <content type="text"><![CDATA[5:00-5:10 新的一天就此开始，请笑对每一天。关爱自己和身边的人。缓慢的起床穿衣服(尽量不要快速起床，容易对心脏等器官造成较大的冲击）。英国威斯敏斯特大学的研究人员发现，那些在早上5:22―7:21 分起床的人，其血液中有一种能引起心脏病的物质含量较高，因此，在7:21之后或者5点之前起床对身体健康更加有益。 打开台灯。“一醒来，就将灯打开，这样将会重新调整体内的生物钟，调整睡眠和醒来模式。”拉夫堡大学睡眠研究中心教授吉姆·霍恩说。喝一杯水。水是身体内成千上万化学反应得以进行的必需物质。早上喝一杯清水，可以补充晚上的缺水状态。 5:10-5:40 在早饭之前刷牙。“在早饭之前刷牙 可以防止牙齿的腐蚀，因为刷牙之后，可以在牙齿外面涂上一层含氟的保护层。要么，就等早饭之后半小时再刷牙。”英国牙齿协会健康和安全研究人员戈登·沃特金斯说。 5:40-7:00 吃点饼干喝点水，整理仪容仪表去健身房听听音乐，跑跑步。等待6点左右的日出。然后到一楼看看书 7:00-7:30 吃早饭。“早饭必须吃，因为它可以帮助你维持血糖水平的稳定。”伦敦大学国王学院营养师凯文·威尔伦说。早饭可以吃燕麦粥等，这类食物具有较低的血糖指数。同时这个时间段也是小肠吸收营养的最佳时段。 7:30-8:00 整理仪表，上班打卡。喝杯水，看看今日头条。 8:00-10:00 避免运动。来自布鲁奈尔大学的研究人员发现，在早晨进行锻炼的运动员更容易感染疾病，因为免疫系统在这个时间的功能最弱。步行上班。马萨诸塞州大学医学院的研究人员发现，每天走路的人，比那些久坐不运动的人患感冒病的几率低25%。然后开始一天中最困难的工作或者学习一下（因为这段时间是学习的一个比较好的时间）。纽约睡眠中心的研究人员发现，大部分人在每天醒来的一两个小时内头脑最清醒。 10:00-11:00 让眼睛离开屏幕休息一下。如果你使用电脑工作，那么每工作一小时，就让眼睛休息3分钟。 11:00-12:00 吃点水果。这是一种解决身体血糖下降的好方法。吃一个橙子或一些红色水果，这样做能同时补充体内的铁含量和维生素C含量。 12:00-12:30 在面包上加一些豆类蔬菜。你需要一顿可口的午餐，并且能够缓慢地释放能量。“烘烤的豆类食品富含纤维素，番茄酱可以当作是蔬菜的一部分。”维伦博士说。 12:30-13:00 午休一小会儿。雅典的一所大学研究发现，那些每天中午午休30分钟或更长时间，每周至少午休3次的人，因心脏病死亡的几率会下降37%。 13:00-16:00 起床清醒一下，开始下午的工作。 16:00-17:00 喝杯酸奶。这样做可以稳定血糖水平。在每天三餐之间喝些酸牛奶，有利于心脏健康。 17:00-17:30 晚餐少吃点。晚饭吃太多，会引起血糖升高，并增加消化系统的负担，影响睡眠。晚饭应该多吃蔬菜，少吃富含卡路里和蛋白质的食物。吃饭时要细嚼慢咽。 17:30-18:00 根据体内的生物钟，这个时间是运动的最佳时间，舍菲尔德大学运动学医生瑞沃·尼克说。 18:00-20:30 建议的学习时间。 20:30-21:00 洗脸刷牙并洗个热水澡。“体温的适当降低有助于放松和睡眠。”拉夫堡大学睡眠研究中心吉姆·霍恩教授说。 21:00-21:30 上床睡觉。如果你早上5点起床，现在入睡可以保证你享受7.5个小时充足的睡眠。而且这个时间段是免疫系统(淋巴)排毒时间，此段时间应安静或听音乐准备入睡。 21:30-23:00 如果您按照这份[作息时间表]作息的话，建议您尽快入睡。 23:00-5:00 当浏览器自动带你来到这段，相信时间已经不早了。切记不可超过12点睡觉，因为那样即使休息够8小时，那也无济于事，一般还是会觉得身体不舒服或者难受。而且最总要的是据说这也是引发心脏病的一个重要原因。但现实生活中多少会有些不如意的事情，也没必要完全遵守 [作息时间表]，只要持之以恒，尽量培养起自己的作息习惯就好。23:00-1:00 肝的排毒，需在熟睡中进行。0:00-4:00 脊椎造血时段，必须熟睡，不宜熬夜。1:00-3:00 胆的排毒，亦同。3:00-5:00 肺的排毒。此即为何咳嗽的人在这段时间咳得最剧烈，因排毒动作已走到肺；不应用止咳药，以免抑制废积物的排除。关于吃早餐。疗病者最好早吃，在6点半前，养生者在7点半前，不吃早餐者应改变习惯，即使拖到9、10点吃都比不吃好。 转载地址]]></content>
      <categories>
        <category>慢生活</category>
      </categories>
      <tags>
        <tag>慢生活</tag>
        <tag>作息时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步锁Synchronized及实现原理]]></title>
    <url>%2F20171129-synchronized%2F</url>
    <content type="text"><![CDATA[同步的基本思想为了保证共享数据在同一时刻只被一个线程使用，我们有一种很简单的实现思想: 在共享数据里保存一个锁 ，当没有线程访问时，锁是空的。当有第一个线程访问时，就在锁里保存这个线程的标识 并允许这个线程访问共享数据。在当前线程释放共享数据之前，如果再有其他线程想要访问共享数据，就要等待锁释放。 在共享数据里保存一个锁 在锁里保存这个线程的标识 其他线程访问已加锁共享数据要等待锁释放 Jvm同步的实现jvm中有以下三种锁(由上到下越来越“重量级”)： 偏向锁 轻量级锁 重量级锁 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。 偏向锁根据轻量级锁的实现，我们知道虽然轻量级锁不支持“并发”，遇到“并发”就要膨胀为重量级锁，但是轻量级锁可以支持多个线程以串行的方式访问同一个加锁对象。 比如A线程可以先获取对象o的轻量锁，然后A释放了轻量锁，这个时候B线程来获取o的轻量锁，是可以成功获取得，以这种方式可以一直串行下去。 之所以能实现这种串行，是因为有一个释放锁的动作。那么假设有一个加锁的java方法，这个方法在运行的时候其实从始至终只有一个线程在调用，但是每次调用完却也要释放锁，下次调用还要重新获得锁。 那么我们能不能做一个假设：“假设加锁的代码从始至终就只有一个线程在调用，如果发现有多于一个线程调用，再膨胀成轻量级锁也不迟”。这个假设，就是偏向锁的核心思想。 偏向锁依赖了一种叫做CAS(compare and swap)的操作。 轻量级锁 JDK 1.6中默认是开启偏向锁和轻量级锁的，我们也可以通过-XX:-UseBiasedLocking来禁用偏向锁。 轻量级锁的核心思想就是“被加锁的代码不会发生并发，如果发生并发，那就膨胀成重量级锁(膨胀指的锁的重量级上升，一旦升级，就不会降级了)”。 轻量级锁依赖了一种叫做CAS(compare and swap)的操作。 重量级锁Synchronized 原理我们直接参考JVM规范中描述：每个对象有一个监视器锁（monitor）。 当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下： 如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者。 如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1. 如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权。 Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象， 这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。 Synchronized是通过对象内部的一个叫做监视器锁（monitor）来实现的。 但是监视器锁本质又是依赖于底层的操作系统的互斥锁（Mutex Lock）来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。 因此，这种依赖于操作系统互斥锁（Mutex Lock）所实现的锁我们称之为“重量级锁”。 术语定义 术语 英文 说明 CAS Compare and Swap 比较并设置。用于在硬件层面上提供原子性操作。在 Intel 处理器中，比较并交换通过指令cmpxchg实现。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 总结 本文重点介绍了JDk中采用轻量级锁和偏向锁等对Synchronized的优化， 但是这两种锁也不是完全没缺点的，比如竞争比较激烈的时候，不但无法提升效率，反而会降低效率，因为多了一个锁升级的过程，这个时候就需要通过-XX:-UseBiasedLocking来禁用偏向锁。下面是这几种锁的对比： 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块场景。 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 如果始终得不到锁竞争的线程使用自旋会消耗CPU。 追求响应时间。同步块执行速度非常快。 重量级锁 线程竞争不使用自旋，不会消耗CPU。 线程阻塞，响应时间缓慢。 追求吞吐量。同步块执行速度较长。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap实现原理分析]]></title>
    <url>%2F20171127-HashMap%2F</url>
    <content type="text"><![CDATA[HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 内部实现搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 数据底层具体存储的是什么？从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。 具体hash算法的原理我们不深入讨论，有兴趣的同学可以参考https://tech.meituan.com/java-hashmap.html我们只要知道我们通过hash方法可以得到对象所在数组的下标。 我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; 主要就是一下几个字段： 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考笔者的文章 功能实现-方法HashMap的内部功能实现很多，本文主要从put方法的详细执行、扩容过程具有代表性的点深入展开讲解。 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下:1234public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125;&#125; 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 Map中各实现类的总结Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) JDK1.8引入红黑树大程度优化了HashMap的性能。 (4) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。 转载地址:查看原文]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆(heap)和栈(stack)有什么区别?]]></title>
    <url>%2F20171122-heap-stack%2F</url>
    <content type="text"><![CDATA[简单的可以理解为： heap：是由malloc之类函数分配的空间所在地。地址是由低向高增长的。 stack：是自动分配变量，以及函数调用的时候所使用的一些空间。地址是由高向低减少的。 一个由c/C++编译的程序占用的内存分为以下几个部分1、栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。2、堆区（heap） — 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。3、全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放4、文字常量区 —常量字符串就是放在这里的。 程序结束后由系统释放5、程序代码区—存放函数体的二进制代码。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子操作的实现原理]]></title>
    <url>%2F20171117-atomic%2F</url>
    <content type="text"><![CDATA[原子(atomic)本意是”不能被进一步分割的最小粒子”,而原子操作(atomic operation)意为”不可被中断的一个或一系列操作”. 处理器如何实现原子操作(1) 使用总线锁保证原子性 如果多个处理器同时对共享变量进行读写操作,那么共享变量就会被多个处理器同时进行操作,这样读写操作就不是原子的,操作完之后共享变量的值会和期望的不一致. 所谓总线锁就是使用处理器提供的一个LOCK#信号,当一个处理器在总线上输出次信号时,其他处理器的请求将被阻塞住,那么该处理器可以独占共享内存. (2) 使用缓存锁保证原子性 所谓”缓存锁”是内存区域如果被缓存在处理器的缓存中,并且在Lock操作期间被锁定,那么当它执行锁操作回写到内存时,处理器不在总线上声言LOCK#信号,而是修改内部的内存地址,并允许它的缓存一致性机制来保证原子性,因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据,当其他处理器回写已被锁定的缓存行的数据时,会使缓存行无效.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用栈实现队列]]></title>
    <url>%2F20171114-stack%2F</url>
    <content type="text"><![CDATA[问题描述 使用两个栈实现一个队列，实现pop方法和push方法，存储元素为int数据 思路 使用stackA做数据存储，使用stackB做临时数据中转。pop时，将stackA的数据转到stackB中，然后pop一个出来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package main.generic;import java.util.Stack;/** * @author: kevin * @date: 2017/11/14 * @description: 栈实现自定义队列 * * 用两个栈模拟一个队列 * A为插入栈--模拟入队列，B为弹出栈--模拟出队列 * (1)入队列，即入栈A * (2)出队列，B栈为空，则A栈元素全部出栈并入栈B，再从B出栈 * B栈不为空，从B出栈 */public class StatckToQueue &#123; private Stack stackA = new Stack(); private Stack stackB = new Stack(); //元素入队列--压入A栈 public void push(int value)&#123; stackA.push(value); &#125; //元素出队列--从B栈弹出 public int pop()&#123; //如果弹出栈B为空，则把A栈中的元素全部压入B栈 if (stackB.empty())&#123; while (!stackA.isEmpty())&#123; stackB.push(stackA.pop()); &#125; &#125; //B栈中元素出栈 return (int) stackB.pop(); &#125; //判断队列是否为空 private boolean isEmplty() &#123; if(stackA.empty() &amp;&amp; stackB.empty())&#123; return true; &#125;else&#123; return false; &#125; &#125; public static void main(String[] args) &#123; StatckToQueue Q = new StatckToQueue(); //元素入队列 System.out.print("元素入队列："); for(int i = 0; i &lt; 3; i++) &#123; Q.push(i); System.out.print(i +" "); &#125; System.out.println(); //元素出队列 System.out.print("元素出队列："); for(int i = 0; i &lt; 3; i++) &#123; int val = Q.pop(); System.out.print(val +" "); &#125; System.out.println(); &#125; 打印结果: 元素入队列：0 1 2 元素出队列：0 1 2 &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volatile的实现原理]]></title>
    <url>%2F20171113-volatile%2F</url>
    <content type="text"><![CDATA[将volatile修饰的java代码转换成汇编代码，进行写操作的时候会多出以lock前缀的汇编代码。lock前缀的指令在多核处理器下会引发两点： 将当前处理器缓存行的数据写回到系统内存。 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 volatile的两条实现原则： Lock前缀指令会引起处理器缓存回写到内存。 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。 锁的状态：锁的级别从低到高依次是：无锁状态 -&gt; 偏向锁状态 -&gt; 轻量级锁状态 -&gt; 重量级锁状态 这几个状态随着竞争情况逐渐升级,锁可以升级但不能降级,意味着偏向锁升级成轻量级锁后不能降级偏向锁,]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Let's Encrypt给网站配置HTTPS]]></title>
    <url>%2F20171110-web-https%2F</url>
    <content type="text"><![CDATA[前言 每个网站都应该用 HTTPS，就算是全静态站点也同样如此，运营商劫持严重干扰访问者的体验 有几项技术可以提高 HTTPS 的性能，包括 Strict Transport Security，TLS False Start 和 HTTP/2 ，这些技术让 HTTPS 速度不慢，某些情况下会甚至更快 HTTPS 针对个人单个（或者几个）域名的使用来说，已经是免费的 配置和维护 HTTPS 异常简单，Let’s Encrypt 这个项目通过自动化把事情简单化了 推荐使用 Let’s Encrypt。StartSSL 的免费证书有效期是1年，1年后需要手动更换。配置过程还挺麻烦的。更推荐 Let’s Encrypt，虽然有效期只有3个月，但可以用 certbot 自动续期，完全不受影响。而且 Let’s Encrypt 因为有了 certbot 这样的自动化工具，配置管理起来非常容易。 Let’s Encrypt证书Let’s Encrypt 证书生成不需要手动进行，官方推荐 certbot 这套自动化工具来实现。3步轻松搞定： 下载安装 certbot (Let’s Encrypt项目的自动化工具) 创建配置文件 执行证书自动化生成命令 下面的教程运行在 Ubuntu 16.04 上，其他操作系统也大同小异。你可以在 certbot 网站上，选择你的 Web Server 和 操作系统，就能看到对应的安装和配置教程。 安装1. 安装Certbot 首先第一步使用Let’s Encrypt获取一个SSL证书,并在服务器上安装Certbot软件 由于Cerbot开发者包含了Unbutu最新版本的repository,因此我们可以使用它的repository替代Ubuntu的. (1) add the repository 1sudo add-apt-repository ppa:certbot/certbot (2) pick up the new repository’s package information 1sudo apt-get update (3) And finally, install Certbot’s Nginx package with apt-get. 1sudo apt-get install python-certbot-nginx Certbot现在已经安装好了,但是为了在Nginx上配置SSL,我们还需要验证一些Nginx的配置信息 2. 配置Nginx certbot能够在Nginx上自动配置SSL证书,但是它需要能找到正确的server模块在你的配置中.检查你的server_name配置是否是你要配置证书的域名 如果你没有Nginx的安装经验,建议你直接修改Nginx的默认配置 1sudo vim /etc/nginx/sites-available/default 找到servername这一行,然后替换’‘换成你的域名 1server_name example.com www.example.com; 保存后,可以验证你的配置是否正确 1sudo nginx -t 验证没有错误的话,重新加载Nginx的配置 1sudo systemctl reload nginx 3. 打开防火墙允许Https通过查看防火墙当前的配置 1sudo ufw status 如果你的配置像下面的话,意思是只允许HTTP服务通过 123456789OutputStatus: activeTo Action From-- ------ ----OpenSSH ALLOW Anywhere Nginx HTTP ALLOW Anywhere OpenSSH (v6) ALLOW Anywhere (v6) Nginx HTTP (v6) ALLOW Anywhere (v6) 添加HTTPS服务通过 12sudo ufw allow 'Nginx Full'sudo ufw delete allow 'Nginx HTTP' 检查一下配置信息 1sudo ufw status 123456789OutputStatus: activeTo Action From-- ------ ----OpenSSH ALLOW AnywhereNginx Full ALLOW AnywhereOpenSSH (v6) ALLOW Anywhere (v6)Nginx Full (v6) ALLOW Anywhere (v6) 下面准备运行Certbot,然后拉取我们的证书 4. 获取SSL证书 certbot通过多种插件提供了多种方式获取SSL证书. 这个Nginx 插件可以帮助我们重新配置Nginx和重新加载配置 1sudo certbot --nginx -d example.com -d www.example.com 使用—nginx 插件运行certbot ,使用-d 指定生成证书的域名 接下来会看到 123456789OutputPlease choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access.-------------------------------------------------------------------------------1: No redirect - Make no further changes to the webserver configuration.2: Redirect - Make all requests redirect to secure HTTPS access. Choose this fornew sites, or if you're confident your site works on HTTPS. You can undo thischange by editing your web server's configuration.-------------------------------------------------------------------------------Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 输入’2’配置所有的请求都走HTTPS服务. 5. 验证Certbot自动更新证书 Let’s Encrypt’s 的证书默认有效时间是9天,使用cerbot可以自动2-3天更新一次证书 为了测试更新的进度,你可以运行cerbot命令: 1sudo certbot renew --dry-run 只要是没报错,设置就是成功了.如果cerbot后面更新证书或者加载Nginx配置失败了, Let’s Encrypt 会发送邮件通知你,警告你的证书已经过期了. 至此所有的配置都完成了,你可以访问你的域名,是不是变成了Https服务了. 哈哈 写得有点累了,希望文章能对你有帮助. 参考文献: https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04 https://certbot.eff.org/docs/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>https</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双亲委派模型]]></title>
    <url>%2F20171103-classloader%2F</url>
    <content type="text"><![CDATA[双亲委派模型是java类加载器所使用的模型. 双亲委派模型的工作过程：如果一个类加载器收到了类加载器的请求.它首先不会自己去尝试加载这个类.而是把这个请求委派给父加载器去完成.每个层次的类加载器都是如此. 因此所有的加载请求最终都会传送到Bootstrap类加载器(启动类加载器)中.只有父类加载反馈自己无法加载这个请求(它的搜索范围中没有找到所需的类)时.子加载器才会尝试自己去加载. 双亲委派模型执行流程 双亲委派模型的好处 java类随着它的加载器一起具备了一种带有优先级的层次关系. 例如类java.lang.Object,它存放在rt.jart之中.无论哪一个类加载器都要加载这个类.最终都是双亲委派模型最顶端的Bootstrap类加载器去加载.因此Object类在程序的各种类加载器环境中都是同一个类.相反.如果没有使用双亲委派模型.由各个类加载器自行去加载的话.如果用户编写了一个称为“java.lang.Object”的类.并存放在程序的ClassPath中.那系统中将会出现多个不同的Object类.java类型体系中最基础的行为也就无法保证.应用程序也将会一片混乱. 双亲委派的代码实现(在ClassLoader类中的loadClass中)12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 逻辑：先检查是否已经被加载过.若没有加载则调用父加载器的loadClass()方法.若父加载器为空则默认使用Bootstrap类加载器作为父加载器.若父加载失败.抛出ClassNotFoundException异常后再调用自己的findClass()方法进行加载]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka]]></title>
    <url>%2F20171101-eureka%2F</url>
    <content type="text"><![CDATA[Eureka 是什么?Eureka是一个基于REST(表述性状态传递)的服务,主要用在AWS云定位服务中,目的是负载均衡和中间层故障转移服务.我们称之为Eureka服务. Eureka也配备了一个基于JAVA的客户端组件,Eureka客户端, 这使得服务的交互更加容易.这个客户端同样内置了一个负载均衡器,可以进行基本的循环负载均衡.在Netflix上,一个更为复杂的Eureka提供加权负载均衡基于多种因素如流量,资源使用错误条件等,以提供更高的弹性.]]></content>
      <categories>
        <category>springcloud</category>
      </categories>
      <tags>
        <tag>springcloud</tag>
        <tag>REST服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA架构图谱]]></title>
    <url>%2F20170831-java-architecture-diagram%2F</url>
    <content type="text"><![CDATA[史上最全的JAVA架构图谱]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome-plugins]]></title>
    <url>%2F20170820-chrome-plugins%2F</url>
    <content type="text"><![CDATA[推荐一些平时好用的Chrome插件。 uBlock Origin 浏览网页的时候广告屏蔽插件是必备的，相比知名度比较高的AdBlock而言，其实uBlock对于广告屏蔽效果要更加好，同时也可以很方便的进行个性化自定义。 Awesome ScreenShot Awesome ScreenShot是一个非常强大的截图插件，支持滚动截图，甚至还可以截图Chrome之外的其他应用，截图之后的对于图片的编辑也是非常的好用，并且也支持录制屏幕。Awesome ScreenShot Pocket 在电脑上看到好的文章，想一键同步到手机上方便在碎片化的时间进行浏览？那么Pocket是一个很好的选择，在移动端的导出格式也是非常值得称赞。 划词翻译对于经常要浏览英文文档的童鞋来说，遇到不熟悉的单词跳转到另外一个软件或者网页中去重新查单词的过程往往整个思路都会被打断，使用了划词翻译当选中一个单词之后，会跳出一个可选的翻译按钮，同时也支持在特定的网页禁止使用这个插件。 QR Code Extension 有时候遇见一些好的内容想分享到朋友圈让更多的朋友受益，一般我们都是在收藏里输入这个链接保存后再在微信中打开这个网页然后才能分享到朋友圈，整个过程非常麻烦到了最后我们分享的欲望都没有了:P，使用QR Code Extension可以直接在PC端生成对应的二维码，微信扫描之后直接打开网页一键分享。 Momentum颜控必备，受够了Chrome单调的新标签页的朋友可以试一试Momentum插件，对于互联网行业的从业者来说，每天在浏览器下的时间比较长，Momentum也支持Todo列表，可以列出一天的代办事项。当然如果你是一个Vimer，这些东西我们从来都不需要:P，不过这应该符合大多数用户的需求。 EvernoteEvernote是一个非常优秀的笔记管理工具，这在一定程度上得益于它在很多应用中优秀的第三方扩展插件，而且搜索功能也是非常的强大，唯一的遗憾就是官方发布的版本不支持markdown形式的笔记记录，这也可能与大多数用户的使用习惯相关。 使用Evernote在Chrome下提供的插件，你可以一键将当前正在浏览的文章或者文档很快捷的保存到Evernote中。同时Evernote的插件也支持对当前正在浏览的文章开启阅读模式，过滤掉除文章之外的信息，专注于阅读。 One-Click Extensions ManagerChrome下如果同时开多个插件是非常消耗资源的，而跳转到二级菜单下面去管理插件太过麻烦，使用One-Click Extensions Manager可以直接很好的管理当前安装的所有插件。 One Tab有时候我们在浏览器中打开了很多的标签页，但是有时候当关闭电脑我们平常可能会需要对这些网页一个一个的收藏这些标签页，下次登录的时候需要一个一个打开，这显然是非常低效的，使用One Tab可以很好的解决这个问题。当我们要登出电脑的时候，点击one tab可以自动保存当前打开的所有网页，当我们下次登录的时候one tab可以帮我们恢复我们上次保存的网页。 TampermonkeyTampermonkey上面提供了很多有趣的脚本，比如在web端使用无限速的百度云，免分享密码下载各大网盘的文件，直接下载Youtube上的视频，导出Instangram上的图片。这是官网的地址，大家可以根据自己的需求到上面去找，如果你懂一点前端开发也可以很轻松的定制一些自己的小脚本。 Download PlusChrome默认的下载按钮是一个二级按钮使用起来很不方便，可以通过安装Download Plus来解决这个痛点，一键管理所有下载的文件。 Last Password不想在多个网站之间使用一个密码，同时又担心密码太多太复杂了又记不住，试一试Last Password插件吧便捷又安全。 VimiumVimium是装逼必备的，全键盘无鼠标的在浏览器中浏览网页，如果你是程序员的话也可以很好的定制相关的规则快捷键，至少装了Vimium之后Chrome自带的快捷键我就基本就没怎么用过。 New Tong Wen TangNew Tong Wen Tang可以自动将网页中的繁体字转换为简体字，比如有时候阅读一些维基百科上的文档的时候，繁体文档的资料相比简体要多很多。 Octotree这是一个为程序员提供的GitHub插件，在GitHub上浏览项目的时候，通过Octotree可以自动为我们生成项目的目录树。]]></content>
      <categories>
        <category>plugins</category>
      </categories>
      <tags>
        <tag>chrome</tag>
        <tag>plugin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SonarQube 代码质量管理平台的搭建]]></title>
    <url>%2F20170615-SonarQube%2F</url>
    <content type="text"><![CDATA[背景： 由于公司项目用的是自己的内部框架，没有引入spring，例如获取数据库的session等操作，都是需要手动操作的，操作多了难免会造成session的未关闭等问题。上周就是因为session的为关闭导致应用运行一段时间后，莫名的不再接受任何请求和处理，也没有任何的异常信息，就像卡住一样。因此排查问题浪费了很多时间，最后检查代码才发现有一处因为数据库的session未关闭的原因造成的。其实如果用到了代码检查工具，这些细节问题是完全可以避免的。 这里推荐使用开源的SonarQube质量管理平台工具 下面是搭建步骤：1、准备环境 jdk1.8 mysql5.6+ 2、 安装mysql并创建数据库sonar1234567891011# mysql -u root -p# mysql&gt; CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci;# mysql&gt; CREATE USER &apos;sonar&apos; IDENTIFIED BY &apos;sonar&apos;;# mysql&gt; GRANT ALL ON sonar.* TO &apos;sonar&apos;@&apos;%&apos; IDENTIFIED BY &apos;sonar&apos;;# mysql&gt; GRANT ALL ON sonar.* TO &apos;sonar&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;sonar&apos;;# mysql&gt; FLUSH PRIVILEGES; 3、 下载安装包服务端工具：sonarqube:http://www.sonarqube.org/downloads/ 客户端工具sonar-runner:http://repo1.maven.org/maven2/org/codehaus/sonar/runner/sonar-runner-dist/2.3/sonar-runner-dist-2.4.zip 4、 安装SonarQube 第一步： 将下载的SonarQube解压安装到/usr/local 目录下。具体步骤如下： 12345# wget -c http://downloads.sonarsource.com/sonarqube/sonarqube-5.1.1.zip# unzip sonarqube-5.1.1.zip# mv sonarqube-5.1.1 /usr/local/ 第二步： 配置环境变量 123456789vim + /etc/profile`添加SONAR_HOME=/usr/local/sonarqube-5.1.1 export SONAR_HOME保存退出并使配置生效source /etc/profile 第三步： 修改配置文件sonar.properties 123456789# vim /usr/local/sonarqube-5.1.1/conf/sonar.properties打开后，找到sonar.host.url=http://192.168.1.168:9000sonar.jdbc.username=rootsonar.jdbc.password=123456sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&amp;useConfigs=maxPerformancesonar.web.host=0.0.0.0sonar.web.context=/sonar.web.port=9000 第四步： 修改配置文件wrapper.conf wrapper.java.command=/usr/local/sonar/jdk/bin/java 注意：把wrapper.conf中的wrapper.java.command修改成jdk1.8路径否则会找系统自带jdk版本的命令执行，启动的时候可能报错 , bin后面还需要加上/java 第五步： 启动服务 # /usr/local/sonarqube-5.1.1/bin/linux-x86-64/sonar.sh start &nbsp;&nbsp;另外，启动/停止/重启命令如下： 12345678910111213# ./sonar.sh start 启动服务 # ./sonar.sh stop 停止服务 # ./sonar.sh restart 重启服务查看启动日志:# tail -f /usr/local/sonarqube-5.6.6/logs/sonar.log关闭命令:# ./sonar.sh stop登录：http://localhost:9000默认密码:admin/admin 第六步： 访问SonarQube Web管理界面。如果能够看到这个界面证明SonarQube安装成功啦。 注：我这里访问的地址是：http://192.168.1.168:9000 5、 安装SonarQube Runner 第一步：将下载的http://repo1.maven.org/maven2/org/codehaus/sonar/runner/sonar-runner-dist/2.4/sonar-runner-dist-2.4.zip解压后放到/usr/local目录下。具体步骤如下： 123# wget -c http://repo1.maven.org/maven2/org/codehaus/sonar/runner/#sonar-runner-dist/2.4/sonar-runner-dist-2.4.zip# unzip sonar-runner-dist-2.4.zip# mv sonar-runner-2.4/ /usr/local/ 第二步：配置环境变量 123456789# vim + /etc/profile添加SONAR_RUNNER_HOME=/usr/local/sonar-runner-2.4/PATH=.:$SONAR_RUNNER_HOME/bin export SONAR_RUNNER_HOME保存并退出 # source /etc/profile 第三步：配置sonar-runner.properties 12345678910# vim /usr/local/sonar-runner-2.4/conf/sonar-runner.properties找到sonar.host.url=http://192.168.1.168sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;amp;characterEncoding=utf8sonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.login=adminsonar.password=admin将前面的#去掉 第四步：运行sonar-runner分析源代码 Sonar官方已经提供了非常全的代码样例便于新手入门用。 下载地址：https://github.com/SonarSource/sonar-examples/archive/master.zip 下载后使用unzip解压。进入java执行sonar-runner命令即可。操作命令如下： 1234# wget -c https://github.com/SonarSource/sonar-examples/archive/master.zip# unzip master.zip# cd sonar-examples-master/projects/languages/java/maven/# sonar-runner 如果能够看到下面的输出信息，证明你的SonarQube Runner安装并配置正确啦。 123456INFO: ------------------------------------------------------------------------INFO: EXECUTION SUCCESSINFO: ------------------------------------------------------------------------Total time: 2:59.167sFinal Memory: 17M/204MINFO: ------------------------------------------------------------------------ 第五步：看看SonarQube的Web界面，是否已经可以看到分析的结果啦。 6、扫描项目 (1) maven环境 (推荐方式，比较方便，扫描完了自动上传结果至sonar服务器中) Maven仓库中就有SonarQube Scanner工具的插件，只要在$M2_HOME/conf/setting.xml文件中添加如下配置 1234567891011121314151617181920212223&lt;pluginGroups&gt; &lt;pluginGroup&gt;org.sonarsource.scanner.maven&lt;/pluginGroup&gt;&lt;/pluginGroups&gt;&lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt;&lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;&lt;/activation&gt;&lt;properties&gt; &lt;sonar.host.url&gt;http://192.168.1.168:9000/sonarqube&lt;/sonar.host.url&gt;&lt;/properties&gt;&lt;/profile&gt; 配置完成后，在项目中，执行mvn sonar:sonar，SonarQube Scanner会自动扫描，根据pom.xml文件得出项目相关信息，不需要自定义sonar-project.properties。扫描完成后就会上传只Sonarqube服务器中。稍后，登陆服务器中就可以看到分析结果了。 (2) 手动扫描 1234567891011121314151617181920212223242526272829303132333435打开要进行代码分析的项目根目录，新建sonar-project.properties文件sonar.projectKey=my:task# this is the name displayed in the SonarQube UIsonar.projectName=My tasksonar.projectVersion=1.0sonar.projectDescription= task 定时任务调度# Path is relative to the sonar-project.properties file. Replace &quot;\&quot; by &quot;/&quot; on Windows.# Since SonarQube 4.2, this property is optional if sonar.modules is set.# If not set, SonarQube starts looking for source code from the directory containing# the sonar-project.properties file.#sources是源文件所在的目录sonar.sources=master/src,slave/srcsonar.binaries=WebRoot/WEB-INF/classes# Encoding of the source code. Default is default system encodingsonar.language=javasonar.my.property=valuesonar.sourceEncoding=UTF-8在项目跟目录执行 输入命令：sonar-runner 7、SonarQube默认是没有安装中文语言包的。如何安装语言包呢？进入SonarQube插件目录，下载语言包即可。步骤如下12# cd /usr/local/sonarqube-5.1.1/extensions/plugins# wget -c http://repo1.maven.org/maven2/org/codehaus/sonar-plugins/l10n/sonar-l10n-zh-plugin/1.8/sonar-l10n-zh-plugin-1.8.jar 这是中文语言包的源码地址：https://github.com/SonarCommunity/sonar-l10n-zh]]></content>
      <categories>
        <category>SonarQube</category>
      </categories>
      <tags>
        <tag>SonarQube</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot-storm-integration]]></title>
    <url>%2F20170601-spring-storm-integration%2F</url>
    <content type="text"><![CDATA[最近在搭建storm与springboot框架的集成，由于本身对storm分布式框架的不熟悉，加上网上能搜到的spring与storm集成的案例也比较少，这一路走来真的是各种坎坷啊，所以也在此总结一下遇到的一些问题，同时也希望能帮到有需要的朋友。 在搭建框架之前，有必要先熟悉一下storm的topology在提交过程中，初始化了什么，实例化了哪些类？ (1) 首先简单说一下，提交topology的流程： 1、首先定义好topology的Config，实例化DrpcSpout、以及Bolt之间的拓扑结构 2、提交Topology之前确认storm的各种服务都启动了，包括zk、nimbus、supervisor、logviewer、ui 3、提交Topology实例给nimbus，这时候调用TopologyBuilder实例的createTopology()方法，以获取定义的Topology实例。在运行createTopology()方法的过程中，会去调用Spout和Bolt实例上的declareOutputFields()方法和getComponentConfiguration()方法，declareOutputFields()方法配置Spout和Bolt实例的输出，getComponentConfiguration()方法输出特定于Spout和Bolt实例的配置参数值对。Storm会将以上过程中得到的实例，输出配置和配置参数值对等数据序列化，然后传递给Nimbus。 4、Worker Node上运行的thread，从nimbus上复制序列化后得到的字节码文件，从中反序列化得到Spout和Bolt实例，实例的输出配置和实例的配置参数值对等数据，在thread中Spout和Bolt实例的declareOutputFields()和getComponentConfiguration()不会再运行。 4、在thread中，反序列化得到一个Bolt实例后，它会先运行Bolt实例的prepare()方法，在这个方法调用中，需要传入一个OutputCollector实例，后面使用该OutputCollector实例输出Tuple 5、接下来在该thread中按照配置数量建立task集合，然后在每个task中就会循环调用thread所持有Bolt实例的execute()方法 6、在关闭一个thread时，thread所持有的Bolt实例会调用cleanup()方法 （2）storm与springboot的集成 1、定义springboot的主入口，也就是Application的启动类 123456@Configuration@EnableAutoConfiguration@ComponentScan(basePackages=&quot;com.demo&quot;)public class Main extends ApplicationObjectSupport &#123;&#125; 2、由于storm的每个bolt都相当于独立的应用，正好每个bolt提供了一个prepare方法，这个prepare方法是在topology提交的时候调用的，这个时候可以把加载spring的过程，放在此处，从而也保证了每个bolt都能获取到Spring的ApplicationContext，有了ApplicationContext，后面的一切都好说了，springboot的任何功能的可以正常使用。废话不说直接贴代码： 123456public void prepare(Map stormConf, TopologyContext context) &#123; super.prepare(stormConf, context); logger.info(&quot;Main start...&quot;); new SpringApplicationBuilder(Main.class).web(false).run(new String[]&#123;&#125;); logger.info(&quot;Main end...&quot;);&#125; 3、获取ApplicationContext前，还需要实现ApplicationContextAware接口，注意一定要加上@Component，spring才会去加载当前类 1234567891011121314151617181920212223242526272829303132@Componentpublic class BeanUtils implements ApplicationContextAware&#123; private static ApplicationContext applicationContext = null; @Override public void setApplicationContext(ApplicationContext arg0) throws BeansException &#123; if (BeanUtils.applicationContext == null) &#123; BeanUtils.applicationContext = arg0; &#125; &#125; // 获取applicationContext public static ApplicationContext getApplicationContext() &#123; return applicationContext; &#125; // 通过name获取 Bean. public static Object getBean(String name) &#123; return getApplicationContext().getBean(name); &#125; // 通过class获取Bean. public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz) &#123; return getApplicationContext().getBean(clazz); &#125; // 通过name,以及Clazz返回指定的Bean public static &lt;T&gt; T getBean(String name, Class&lt;T&gt; clazz) &#123; return getApplicationContext().getBean(name, clazz); &#125;&#125; 4、通过ApplicationContext获取Service实现类,注意Service一定要加上@Service(name=”demoService”)，不加别名的话，会获取不到，你可以试一下。(DemoService) applicationContext.getBean(&quot;demoService&quot;); 到此简单的整合就完成了，重点是每个bolt都需要独立的ApplicationContext，才能获取beans，切入点也就是bolt的prepare()方法中。]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Topology的初始化流程]]></title>
    <url>%2F20170531-storm-life-cycle%2F</url>
    <content type="text"><![CDATA[1、 首先配置好topology，并定义好Spout实例和Bolt实例 2、 在提交Topology实例给Nimbus的过程中，会调TopologyBuilder实例的createTopology()方法，以获取定义的Topology实例。在运行createTopology()方法的过程中，会去调用Spout和Bolt实例上的declareOutputFields()方法和getComponentConfiguration()方法，declareOutputFields()方法配置Spout和Bolt实例的输出，getComponentConfiguration()方法输出特定于Spout和Bolt实例的配置参数值对。Storm会将以上过程中得到的实例，输出配置和配置参数值对等数据序列化，然后传递给Nimbus。 3、在Worker Node上运行的thread，从Nimbus上复制序列化后得到的字节码文件，从中反序列化得到Spout和Bolt实例，实例的输出配置和实例的配置参数值对等数据，在thread中Spout和Bolt实例的declareOutputFields()和getComponentConfiguration()不会再运行。 4、在thread中，反序列化得到一个Bolt实例后，它会先运行Bolt实例的prepare()方法，在这个方法调用中，需要传入一个OutputCollector实例，后面使用该OutputCollector实例输出Tuple 5、接下来在该thread中按照配置数量建立task集合，然后在每个task中就会循环调用thread所持有Bolt实例的execute()方法 6、在关闭一个thread时，thread所持有的Bolt实例会调用cleanup()方法 不过如果是强制关闭，这个cleanup()方法有可能不会被调用到]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm 启动相关命令]]></title>
    <url>%2F20170531-start-storm-command%2F</url>
    <content type="text"><![CDATA[在启动storm之前要确保nimbus和supervisor上的Zookeeper已经启动1.查看zk的状态：./zkServer.sh status 2.如果zk没有开启，将nimbus和supervisor的zk开启./zkServer.sh start 3.启动nimbus（切换到storm的bin目录下）nohup ./storm nimbus &amp; 4.启动supervisornohup ./storm supervisor &amp; 4.启动storm UInohup ./storm ui &amp; 在浏览器中输入ip:8080/index.html进入storm UI界面（注意端口不一定是8080，注意配置）]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github提交代码到开源项目]]></title>
    <url>%2F20170512-push-request%2F</url>
    <content type="text"><![CDATA[首先,在github中把开源项目fork到自己的仓库1234$ git clone https://github.com/kevincefang/test.git$ cd test$ git config user.name "yourname"$ git config user.email "your email" 更新内容提交后,并推送到自己的仓库1234$ #do some change on the content$ git add .$ git commit -m "Fix issue"$ git push 最后，在 GitHub 网站上提交 pull request 即可。 另外，建议定期使用项目仓库内容更新自己仓库内容。 12345$ git remote add upstream https://github.com/kevincefang/test.git$ git fetch upstream$ git checkout master$ git rebase upstream/master$ git push -f origin master]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F20150203-hello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo s More info: Server Generate static files1$ hexo g More info: Generating Sync images to QiniuYun1$ hexo qiniu sync Deploy to remote sites1$ hexo d More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
